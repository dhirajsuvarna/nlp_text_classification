{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-ULMFit-IMDB-Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhirajsuvarna/nlp_text_classification/blob/master/learning/NLP_ULMFit_IMDB_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B1ZDGD5XyjX"
      },
      "source": [
        "# Sentiment Analysis - IMDB\n",
        "\n",
        "Reference code - https://github.com/fastai/course-nlp/blob/master/review-nlp-transfer.ipynb\n",
        "\n",
        "This notebook uses `version 1` for `fastai`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-ET8Hy1FRN0"
      },
      "source": [
        "from fastai.text import *"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYpZTmCfBWf8",
        "outputId": "9bc9c166-a13c-45f7-dee5-68133f54464e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# recent version of fastai is 2.* - which needs to be installed in the colab seperately \n",
        "import fastai\n",
        "fastai.__version__"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.0.61'"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAWTJddtk1Pi"
      },
      "source": [
        "## Get the data of IMDB \n",
        "\n",
        "This is a sample version of the IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNz-O50tk9hB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf73aa33-c9e2-4236-9a81-af78852a4bbb"
      },
      "source": [
        "print(f\"imdb sample path: {URLs.IMDB_SAMPLE}\")\n",
        "path = untar_data(URLs.IMDB_SAMPLE)\n",
        "print(f\"path:{path}\")"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imdb sample path: http://files.fast.ai/data/examples/imdb_sample\n",
            "path:/root/.fastai/data/imdb_sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP31vVhjmeg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f1473e5-e753-41c9-9729-1d1e8b97d2f0"
      },
      "source": [
        "df = pd.read_csv(path/'texts.csv')\n",
        "print(df.head(5))\n",
        "print(df.shape)\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      label                                               text  is_valid\n",
            "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
            "1  positive  This is a extremely well-made film. The acting...     False\n",
            "2  negative  Every once in a long while a movie will come a...     False\n",
            "3  positive  Name just says it all. I watched this movie wi...     False\n",
            "4  negative  This movie succeeds at being one of the most u...     False\n",
            "(1000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FImzuJMY_20M",
        "outputId": "a4f6090a-a531-41cb-dcc0-7d62bfb4d190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train and Validation split in the dataset\n",
        "print(df[df['is_valid'] == True].shape)\n",
        "print(df[df['is_valid'] == False].shape)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 3)\n",
            "(800, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGgQsf9Ntbyp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "63d2df59-a9ea-4b11-980b-fb4fbf914be5"
      },
      "source": [
        "# TextLMDataBunch isn't supported by fastai v2 \n",
        "\n",
        "data_lm = TextLMDataBunch.from_csv(path, 'texts.csv')\n",
        "print(type(data_lm))\n",
        "# print(data_lm)\n",
        "\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'fastai.text.data.TextLMDataBunch'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqbVlDZ7C-tD"
      },
      "source": [
        "**`TextLMDataBunch.from_csv`**\n",
        "\n",
        "Create a TextDataBunch from texts in csv files. kwargs are passed to the dataloader creation.\n",
        "\n",
        "This method will look for csv_name, and optionally a test csv file, in path. These will be opened with header, using delimiter. You can specify which are the text_cols and label_cols; by default a single label column is assumed to come before a single text column. If your csv has no header, you must specify these as indices. If you're training a language model and don't have labels, you must specify the text_cols. If there are several text_cols, the texts will be concatenated together with an optional field token. If there are several label_cols, the labels will be assumed to be one-hot encoded and classes will default to label_cols (you can ignore that argument for a language model). label_delim can be used to specify the separator between multiple labels in a column.\n",
        "\n",
        "You can pass a tokenizer to be used to parse the texts into tokens and/or a specific vocab for the numericalization step (if you are building a classifier from a language model you fine-tuned for instance). Otherwise you can specify parameters such as max_vocab, min_freq, chunksize for the Tokenizer and Numericalizer (processors). Other parameters (e.g. bs, val_bs and num_workers, etc.) will be passed to LabelLists.databunch() documentation) (see the LM data and classifier data sections for more info).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxmOTxLFCEP_"
      },
      "source": [
        "data_lm.save('data_lm_export.pkl')"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL1Q3xSh9uhk"
      },
      "source": [
        "bs=64"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZOUJx2J9wV3"
      },
      "source": [
        "data_lm = load_data(path, 'data_lm_export.pkl', bs=bs)\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmKKnyZQWsVB"
      },
      "source": [
        "torch.cuda.set_device(0)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guqvbjyo99Jb"
      },
      "source": [
        "learn_lm = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5).to_fp16()\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKC_qcOpD2_b",
        "outputId": "c9cccc7a-603e-4f37-eaa4-8f57d035a961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "learn_lm.fit_one_cycle(1, slice(1e-2))\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.333601</td>\n",
              "      <td>3.910637</td>\n",
              "      <td>0.284747</td>\n",
              "      <td>00:32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5HJrupKERpu",
        "outputId": "d225a4eb-cd4a-4dc9-a311-36b2da68b199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learn_lm.unfreeze()\n",
        "learn_lm.fit_one_cycle(5, slice(1e-4,1e-2))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.963475</td>\n",
              "      <td>3.844701</td>\n",
              "      <td>0.290283</td>\n",
              "      <td>00:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.798450</td>\n",
              "      <td>3.861188</td>\n",
              "      <td>0.287232</td>\n",
              "      <td>00:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.467994</td>\n",
              "      <td>3.895121</td>\n",
              "      <td>0.285372</td>\n",
              "      <td>00:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.079474</td>\n",
              "      <td>3.954618</td>\n",
              "      <td>0.280476</td>\n",
              "      <td>00:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.805176</td>\n",
              "      <td>3.977736</td>\n",
              "      <td>0.280833</td>\n",
              "      <td>00:40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjfK8aGPF5QU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f29c3f27-4715-4407-c93b-0b65ce976ca4"
      },
      "source": [
        "learn_lm.predict(\"this movie sucks big time\", n_words=25)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this movie sucks big time , there is as much though about education and re - education . As i say , Survivor seems to not have been'"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0fND4uvFWHK",
        "outputId": "66e9cd9f-dba8-44a6-af15-d9083abf33b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "learn_lm.predict(\"i ate a hot\", n_words=5)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i ate a hot chick , large blah enough'"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI3GMnXTNsEs"
      },
      "source": [
        "learn_lm.save('ft')\n",
        "learn_lm.save_encoder('ft_enc')"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5ySmy6TFz5s",
        "outputId": "123b01c3-eea5-4de1-ac9c-adfcf0f6352c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# location above files are saved\n",
        "!ls /root/.fastai/data/imdb_sample/models"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ft_enc.pth  ft.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIl2bNRrkyrM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47270385-80e4-40b6-ca4b-6d758623dd21"
      },
      "source": [
        "#print(data_lm.train_ds.vocab)\n",
        "print(data_lm.vocab.itos)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxfld', 'xxmaj', 'xxup', 'xxrep', 'xxwrep', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'that', 'this', '\"', \"'s\", '-', '\\n \\n ', 'was', 'as', 'for', 'movie', 'with', 'but', 'film', ')', 'you', '(', 'on', \"n't\", 'are', 'not', 'his', 'he', 'have', 'be', 'one', 'they', 'at', 'all', 'by', 'an', 'from', 'like', 'so', 'who', '!', 'there', 'if', 'just', 'her', 'about', 'or', 'out', \"'\", 'do', 'some', 'has', 'what', 'good', 'more', 'when', '?', 'up', 'she', 'very', 'no', 'their', 'would', 'time', 'were', 'even', 'story', 'really', 'my', 'can', 'had', 'which', 'only', 'see', 'me', '...', ':', 'did', 'we', 'bad', 'into', 'than', 'does', 'well', 'will', 'also', 'first', 'get', 'other', 'people', 'its', 'because', 'could', 'how', 'much', 'been', 'most', 'any', 'make', 'great', 'two', 'made', 'them', '/', 'him', 'then', 'after', 'way', 'too', 'never', 'acting', 'think', ';', 'films', 'better', 'movies', 'character', 'little', 'watch', 'life', 'being', 'where', 'love', 'man', 'many', 'over', 'plot', 'your', 'characters', 'here', 'best', 'end', 'seen', 'know', 'scenes', 'something', 'should', 'through', 'ever', 'say', 'show', 'while', 'such', 'off', 'still', 'back', 'go', 'these', 'scene', 'those', 'though', 'before', 'real', 'why', 'years', 'new', 'watching', 'quite', '10', 'thing', 'actors', 'old', 'actually', 'same', 'nothing', \"'ve\", \"'m\", 'between', '--', 'now', '&', 'makes', 'going', 'look', 'part', 'action', 'work', 'few', 'want', \"'re\", 'pretty', 'find', '*', 'around', 'fact', 'funny', 'director', 'another', 'down', 'without', 'again', 'seems', 'enough', 'both', 'world', 'cast', 'lot', 'point', 'us', 'bit', 'series', 'ca', 'original', 'almost', 'whole', 'got', 'must', 'own', 'every', 'may', 'things', 'thought', 'might', 'probably', 'horror', 'big', 'interesting', 'however', \"'ll\", 'least', 'gets', 'family', 'come', 'each', 'young', 'true', 'feel', 'times', 'right', 'role', 'hard', 'away', 'making', 'day', 'comes', 'fun', 'kind', 'minutes', 'script', 'take', 'girl', 'done', 'always', 'place', 'guy', 'looks', 'saw', 'someone', 'anything', 'shows', 'last', 'long', 'tv', 'american', 'course', 'goes', 'yet', 'far', 'give', 'effects', 'simply', \"'d\", 'book', 'set', '4', 'actor', 'comedy', 'trying', 'star', 'although', 'left', 'together', 'worth', 'takes', 'audience', 'music', 'found', 'shot', 'once', 'believe', 'looking', 'our', 'screen', 'performance', 'ending', 'especially', 'watched', 'sure', 'having', 'since', 'rest', 'sense', 'said', 'special', 'three', '\\x96', 'men', 'worst', 'black', 'dvd', 'wife', '2', 'women', 'john', 'friends', 'instead', 'woman', 'played', 'am', 'read', 'high', 'different', 'plays', 'version', 'everyone', 'put', 'low', 'budget', 'anyone', 'second', 'father', 'main', 'rather', 'poor', 'during', 'next', 'later', 'seem', 'production', 'job', '1', 'maybe', 'boring', 'beautiful', 'completely', 'video', 'performances', 'perhaps', 'others', 'terrible', 'lost', 'war', 'death', 'let', 'couple', 'less', 'half', 'liked', 'start', 'year', 'nice', 'either', 'reason', 'else', 'night', 'house', 'mind', 'kids', 'top', 'need', 'robert', 'try', 'problem', 'fan', 'episode', 'piece', 'play', 'sex', 'wrong', '5', 'girls', 'idea', 'everything', 'entire', 'tell', 'camera', 'came', 'along', 'excellent', 'remember', '3', 'kill', 'often', 'money', 'given', 'throughout', 'sound', 'until', 'case', 'keep', 'town', 'full', 'yes', 'style', 'name', 'called', 'picture', 'help', 'drama', 'fans', 'eyes', 'leave', 'use', 'hollywood', '..', 'moments', 'under', 'definitely', 'used', 'live', 'dialogue', 'late', 'hand', 'already', 'enjoy', 'cinema', 'boy', 'worse', 'stars', 'supposed', 'white', 'attempt', 'awful', 'felt', 'quality', 'fine', 'truly', 'classic', 'writing', 'friend', 'lines', 'small', 'mean', 'getting', 'itself', 'english', 'parts', 'absolutely', 'son', 'home', 'doing', 'child', 'himself', 'despite', 'hope', 'seeing', 'oh', 'blood', 'early', 'james', 'entertaining', 'sort', 'unfortunately', 'lives', 'king', 'able', 'close', 'recommend', 'written', 'history', 'past', 'face', 'title', 'overall', 'gives', 'person', 'matter', 'days', 'waste', 'decent', 'says', 'mother', 'genre', 'turn', 'dull', 'become', 'line', 'heard', 'care', 'word', 'anyway', 'guys', 'car', 'except', 'humor', 'based', 'short', 'tries', 'sometimes', 'wo', 'lead', 'five', 'wonderful', 'beginning', 'daughter', 'flick', 'brilliant', 'light', 'against', 'guess', 'dead', 'killer', 'told', 'sister', 'direction', 'self', 'violence', 'gore', 'human', 'wonder', 'exactly', 'evil', 'hero', 'turns', 'feeling', 'side', 'city', 'god', 'behind', 'works', 'head', 'wants', 'run', 'documentary', 'stupid', 'went', 'dark', 'thinking', 'wanted', 'school', 'viewer', 'cheap', 'release', 'certainly', 'children', 'follow', 'game', 'loved', 'laugh', 'playing', 'local', 'themselves', 'final', 'highly', 'killed', 'ago', 'voice', 'hit', 'lack', 'opinion', 'tells', 'shown', 'red', 'starts', 'level', 'soon', 'roles', 'eye', 'seemed', 'etc', 'relationship', 'enjoyed', 'supporting', 'modern', 'non', 'finally', 'turned', 'several', 'ridiculous', 'view', 'expect', 'interest', 'usually', 'novel', 'perfect', 'slow', 'crap', 'review', 'problems', 'serious', 'saying', 'paul', 'nearly', 'filmed', 'took', 'yourself', 'group', 'mostly', 'example', 'events', 'please', 'hour', 'happens', 'victor', 'de', 'upon', 'ben', 'power', 'order', 'moment', 'gave', 'falls', 'using', 'tale', 'score', 'lady', 'including', 'sad', 'alone', 'episodes', 'general', 'shots', 'number', 'heart', 'horrible', 'george', 'cinematography', 'room', 'mr.', 'simple', 'totally', 'type', 'ok', 'lake', 'becomes', 'editing', 'country', 'sets', 'convincing', 'bunch', 'working', 'huge', 'clearly', 'stories', '7', 'amazing', 'writer', 'dance', 'age', 'living', 'fight', 'body', 'four', 'michael', 'secret', 'similar', 'sexual', 'straight', 'bring', 'art', 'possible', 'today', 'weak', 'water', 'boys', 'apparently', 'somewhat', 'romance', 'york', 'nature', 'talking', 'steve', 'seriously', 'obviously', 'ones', 'myself', 'greatest', 'brother', 'oscar', 'stage', 'woods', 'extremely', 'female', 'wish', 'particularly', 'create', 'happened', 'husband', 'british', 'romantic', 'particular', 'experience', 'knew', 'needs', 'stuff', 'ways', 'japanese', 'cover', 'leads', 'earth', 'finds', 'knows', 'change', 'features', 'subject', 'happy', '6', 'coming', 'difficult', 'spent', 'happen', 'involved', 'ends', 'gone', 'none', 'figure', 'b', 'chance', 'theme', 'kid', 'understand', 'effort', 'released', 'moving', 'chase', 'within', 'team', 'became', 'monster', 'directed', 'mention', 'beyond', 'typical', 're', 'whether', 'recently', 'favorite', 'obvious', 'taken', 'hell', 'doubt', 'killing', 'mentioned', 'major', 'fall', 'easily', 'save', 'across', 'actress', 'form', 'jokes', 'surprised', '8', 'words', 'easy', 'western', 'zombie', 'feature', 'ten', 'cgi', 'luke', 'truth', 'twist', 'herself', 'parents', 'imagine', 'gay', 'disappointed', 'complete', 'talent', 'add', 'devil', 'keeps', 'future', 'hardly', 'talk', 'footage', 'act', 'lee', 'silly', 'list', 'fantastic', 'material', 'strange', 'above', 'due', 'tom', 'usual', 'sit', 'decided', 'rent', 'attention', 'strong', 'whatever', 'named', 'suspense', 'audiences', 'taking', 'indian', 'scary', 'forget', 'certain', 'kept', 'screenplay', 'towards', 'career', 'business', 'points', 'avoid', 'cheesy', 'science', 'door', 'interested', 'enjoyable', 'reality', 'space', 'apart', 'plenty', 'entertainment', 'looked', 'realistic', 'bourne', 'stay', 'known', 'important', 'stop', 'davis', 'setting', 'box', 'hilarious', 'note', 'actual', 'move', 'dialog', 'stand', 'portrayed', 'laughs', 'clear', 'bill', 'jim', 'opening', 'blob', 'soundtrack', '80', 'call', 'christmas', 'company', 'whose', 'richard', 'attempts', 'jane', 'started', 'allen', 'musical', 'giving', 'cut', 'forced', 'thriller', 'predictable', 'lots', 'means', 'break', 'dream', 'dancing', 'sequences', 'former', 'feels', 'average', 'french', 'homer', 'dad', 'rich', 'deal', 'unique', 'dr.', 'indeed', 'question', 'police', 'cool', 'tried', 'died', 'mess', 'rating', 'deserves', 'remake', 'buy', 'adaptation', 'leading', 'west', 'storyline', 'message', 'surprisingly', 'basic', 'caught', 'language', 'television', 'sadly', 'comic', 'south', 'viewing', 'hours', 'baby', 'begin', 'slasher', 'era', 'basically', 'hold', 'showing', 'gun', 'la', 'near', 'single', 'tension', 'possibly', 'david', 'appear', 'missing', 'rock', 'song', 'premise', 'brothers', 'street', 'cold', 'decides', 'crazy', 'beauty', '20', 'emotional', 'viewers', 'america', 'society', 'fairly', 'jack', 'al', 'porn', 'sequence', 'alive', 'third', 'vargas', 'sorry', 'hear', 'meets', 'middle', 'weird', 'german', 'design', 'meaning', 'die', 'elements', 'fake', 'agree', 'sequel', 'pain', 'incredibly', 'directing', 'somehow', 'shop', 'helps', 'period', 'deep', 'fire', 'building', 'crime', 'minute', 'social', 'comment', 'explain', 'flat', 'lame', 'hair', 'public', 'values', 'murder', 'outside', 'cry', 'crew', 'married', 'accent', 'involving', 'casting', 'running', 'project', 'slightly', 'songs', 'further', 'miss', 'eventually', 'unless', 'follows', 'sounds', 'purpose', 'dramatic', 'needed', 'considering', 'mystery', 'believable', 'ability', 'normal', 'comments', 'honestly', 'martin', 'older', 'male', 'stunning', 'tone', 'stick', 'fast', 'clever', 'annoying', 'air', 'agent', 'popular', 'reading', 'narrative', 'kate', 'appears', 'rated', 'nudity', 'nick', 'hands', 'focus', 'dreams', 'among', 'century', 'shooting', 'kills', 'suddenly', 'cartoon', 'writers', 'development', 'value', 'acted', 'win', 'laughing', 'makers', 'begins', 'earlier', 'nor', 'members', 'russian', 'brought', 'total', 'singing', 'joe', 'following', 'office', 'telling', '+', 'barely', 'positive', 'realize', 'image', '30', 'sick', 'creepy', 'waiting', 'road', 'dressed', 'grace', 'bizarre', '15', 'pace', 'ask', 'puts', 'onto', 'thrown', 'arthur', 'large', 'present', 'okay', 'costumes', 'free', 'ghost', 'directors', 'producers', 'concept', 'grade', 'touching', 'runs', 'taste', 'peter', 'recent', 'manages', 'hot', 'intelligence', 'epic', 'burns', 'credit', 'mark', 'pick', 'expectations', 'sitting', 'aware', 'hate', 'willing', 'numerous', 'evening', 'heavy', 'wait', 'front', 'trash', 'jones', 'adventure', 'unlike', 'badly', 'successful', 'revenge', '9', 'previous', 'natural', 'intended', 'animation', 'plain', 'meet', 'somewhere', 'thanks', 'noir', 'moves', 'sam', 'claim', 'return', 'conflict', 'pointless', 'perfectly', 'scott', 'learn', 'commentary', 'situation', 'asks', 'plus', 'filled', 'cat', 'forward', 'control', '%', 'bother', 'poorly', 'kevin', 'loud', 'considered', 'six', 'glad', 'expected', 'built', 'impressive', 'recommended', 'professional', 'leaves', 'ex', 'photography', 'silent', 'post', 'open', 'impact', 'food', 'limited', 'hey', 'wondering', 'animated', 'aspects', 'copy', 'battle', 'besides', 'otherwise', 'drive', 'allow', 'africa', 'biko', 'track', 'bored', 'season', 'terms', 'touch', 'share', 'literally', 'powerful', 'wells', 'continuity', 'cause', 'brings', 'escape', '$', 'various', 'government', 'sexy', 'movement', 'personal', 'doctor', 'background', 'mediocre', 'addition', 'developed', 'worked', 'wrote', 'months', 'loving', 'relationships', 'radio', 'admit', 'inside', 'window', 'mysterious', 'sight', 'turning', 'meant', 'wanting', 'pilot', 'expecting', 'struggle', 'violent', 'wasted', 'memorable', 'villain', 'shame', 'situations', 'evidence', 'dog', 'sci', 'fi', 'wild', 'sees', 'baseball', 'redeeming', 'army', 'fanfan', 'missed', 'trouble', 'naschy', 'yeah', 'tony', 'store', 'credits', 'fantasy', 'quickly', 'ideas', 'vision', 'sub', 'effective', 'william', 'versions', 'success', 'deserve', 'filming', 'animals', 'african', 'disappointment', 'fox', 'changed', 'motion', 'creature', 'theatrical', 'joey', 'latter', 'failed', 'pass', 'party', 'rubbish', 'apartment', 'prince', 'standing', 'leaving', 'speaking', 'zombies', 'common', 'met', 'channel', 'wooden', 'speak', 'born', 'neither', 'joke', 'emotions', 'dying', 'actresses', 's', 'edge', 'perspective', 'result', 'cult', 'showed', 'depth', 'longer', 'beautifully', 'zero', 'pieces', 'famous', 'historical', 'media', 'spoilers', 'freedom', 'willie', 'masterpiece', 'ship', 'brooks', 'odd', 'lose', 'cooper', 'martial', 'mine', 'trip', 'painful', 'childhood', 'steven', 'compared', 'cute', 'anybody', 'garbage', 'imdb', 'rental', 'support', 'location', 'respect', 'murdered', 'brief', 'williams', 'computer', 'excuse', 'vhs', 'locations', 'surprise', 'ugly', 'remains', 'aside', 'ended', 'machine', 'wearing', 'clothes', 'likes', 'makeup', 'mad', 'genius', 'surely', 'event', 'superior', 'beat', 'industry', 'talented', 'cross', 'charm', 'smith', 'washington', 'immediately', 'treasure', 'sky', 'regard', 'force', 'demons', 'creative', 'serial', '90', 'changes', 'details', 'class', 'merely', 'state', 'blue', 'existence', 'humour', 'vampire', 'vampires', 'church', 'brutal', '50', 'feelings', 'site', 'starring', 'bland', 'japan', 'arts', 'wars', 'franchise', 'filmmakers', 'attack', 'plan', 'gold', 'fair', 'land', 'forever', 'current', 'award', 'colors', 'superb', 'solid', 'chris', 'reviews', 'johnny', 'law', 'walk', 'jackson', 'visual', 'astaire', 'crater', 'effect', 'efforts', 'heroine', 'students', 'suggest', 'manner', 'adds', 'captain', 'check', 'received', 'band', 'claire', 'happening', 'identity', 'match', 'dies', 'twenty', 'fell', 'laughter', 'co', 'connection', 'news', 'choice', 'scenery', 'fiction', 'thank', 'lovers', 'winning', 'names', 'frank', 'honest', 'ultimately', 'charles', 'extreme', 'lies', 'charming', 'contains', 'flaws', 'seven', 'issues', 'warning', 'information', 'rare', 'responsible', 'henry', 'friendship', 'teenager', 'community', 'biggest', 'wow', 'images', 'soldiers', 'incredible', 'catch', 'answer', 'books', 'savage', 'stays', 'nicely', 'captured', 'lewis', 'managed', 'likable', 'intelligent', 'bought', 'extras', 'fails', 'chemistry', 'length', 'driving', 'won', 'hall', 'van', 'deserved', 'lying', 'animal', 'legend', 'seagal', 'pure', 'girlfriend', 'thinks', 'candy', 'awesome', 'offer', 'spot', 'personally', 'shirt', 'appeal', 'apparent', 'friday', 'spoiler', 'alien', 'station', 'system', 'bloody', 'color', 'approach', 'pleasure', 'sign', 'student', 'picked', 'recall', 'summer', 'walking', 'warn', 'stewart', 'studio', 'spirit', 'standard', 'pathetic', 'teen', 'reach', 'producer', 'comedies', 'thin', 'entirely', 'boss', 'texas', 'fate', 'write', 'provide', 'confusing', 'whom', 'followed', 'weeks', 'island', 'include', 'humorous', 'stands', 'ground', 'reviewers', 'aspect', 'forgotten', 'tend', 'pacing', 'lover', 'ladies', 'questions', 'falling', 'emotion', '1970s', 'prove', 'stephen', 'loose', 'claims', 'castle', 'conclusion', 'political', 'knowing', 'appeared', 'cars', 'roger', 'themes', 'whatsoever', 'valentine', 'gangster', 'negative', 'hurt', 'justice', 'witch', 'amount', 'trailer', 'mood', 'adult', 'italian', 'fame', 'fellow', 'count', 'opportunity', 'villains', 'reasons', 'context', 'week', 'impression', 'ms.', 'serbian', 'rate', 'national', 'spend', 'soderbergh', 'grew', 'loves', 'twice', 'trust', 'anywhere', 'junior', 'fear', 'u.s.', 'added', 'presence', 'hardy', 'steal', 'wise', 'enemy', 'reminiscent', 'equally', 'edited', 'promising', 'soul', 'consider', 'jason', 'super', 'places', 'fool', 'everybody', 'lugosi', 'somebody', 'impossible', 'stuck', 'continues', 'keeping', 'powers', 'lighting', 'hill', 'batman', 'destroyed', 'appearance', 'semi', 'direct', 'sweet', 'journey', 'inspired', 'artistic', 'dennis', 'nowhere', 'hits', 'memory', 'likely', 'brian', 'tough', 'suicide', 'mouth', 'decade', 'exact', 'fashion', 'unusual', 'finding', 'states', 'welcome', 'jump', 'lovely', 'member', 'wilson', 'amusing', 'standards', 'taylor', 'bottom', 'dirty', 'naked', 'according', 'lets', 'culture', 'leader', 'majority', 'fred', 'teenage', 'portrayal', 'adam', 'date', 'proves', 'games', 'crash', 'survive', 'laughable', 'skip', 'train', 'religious', 'ryan', 'deals', 'began', 'angles', 'thousands', '100', 'danes', '\\x85 ', 'introduced', 'louis', 'pleasant', 'christian', 'bed', 'central', 'forgot', 'magic', 'jean', 'afraid', 'cinematic', 'appreciate', 'gilliam', 'blonde', 'ann', 'hanks', 'daughters', 'subtle', 'judy', 'princess', 'pull', 'carradine', 'thus', 'johnson', 'evelyn', 'geisha', 'scorsese', 'jabba', 'massive', 'bob', 'confused', 'neighborhood', 'allowed', 'fully', 'available', 'starting', 'weight', 'extra', 'par', 'compelling', '1970', 'rarely', 'shocking', 'fifteen', 'ultimatum', 'cia', 'ordinary', 'bond', 'tarzan', 'heads', 'sheriff', 'mary', 'popcorn', 'horribly', 'acceptable', 'pulled', 'offering', 'sucks', 'concerned', 'heck', 'unbelievable', 'attractive', 'field', 'soap', 'opera', 'jr.', 'outstanding', 'hunter', '70', 'f', 'reveal', 'detail', 'disappointing', 'understanding', 'deliberately', 'ray', 'massacre', 'levels', 'created', 'club', 'andy', 'refuses', 'pay', 'phone', 'artist', 'toward', 'bloodbath', 'awkward', 'victims', 'touches', 'hoping', 'rescue', 'moral', 'described', 'marriage', 'marry', 'opposite', 'essentially', 'ill', 'dealing', 'unnecessary', 'mob', 'mask', 'suffer', 'soldier', 'torn', 'o', 'discovered', 'ass', 'keaton', 'mistake', 'personality', 'boat', 'hopefully', 'presented', 'experiences', 'slowly', 'campy', 'racism', 'paxton', 'studios', 'guns', 'han', 'tragic', 'utter', 'treat', 'decide', 'lacks', 'terribly', 'chinese', 'niro', 'double', 'mistaken', 'bringing', 'bits', 'produced', 'rule', 'modesty', 'x', 'ahead', 'sent', 'uses', 'innocent', 'ready', 'ii', 'finest', 'alex', 'victim', 'faces', 'gorgeous', 'mainly', 'ballet', 'howard', 'younger', 'device', 'advice', 'paid', 'deliver', 'border', 'climax', 'snakes', 'blacks', 'redgrave', 'vanessa', 'inner', 'hated', 'serves', 'constantly', 'seemingly', 'throw', 'ed', 'wood', 'disgusting', 'desperate', 'fascinating', 'blown', 'funniest', 'saved', 'college', 'jeff', 'morning', 'exploitation', 'unlikely', 'generally', 'theater', 'folks', 'laughed', 'england', 'model', 'angry', 'spy', 'miles', 'returns', 'hospital', 'moore', 'collection', 'amateurish', 'joy', 'foreign', 'execution', 'scream', 'chose', 'packed', 'dynamic', 'ruined', 'logic', 'feet', 'eat', 'skits', 'sake', 'frankly', 'finish', 'prison', 'alfred', 'engaging', 'learns', 'dear', 'nt', 'sheer', 'realism', 'exception', 'survivors', 'search', 'twists', 'provides', 'atmosphere', 'empty', 'player', 'cynical', 'fresh', 'green', 'grandmother', 'provided', 'becoming', 'proper', 'price', 'record', 'featured', 'commercials', 'disaster', 'pulls', 'boyfriend', 'enters', 'normally', 'sheets', 'statement', 'golden', 'worthwhile', 'doom', 'grand', 'faith', 'potential', 'accept', 'ups', 'leon', 'wives', 'enemies', 'kennedy', 'absolute', 'focused', 'blame', 'finished', 'military', 'author', 'apartheid', 'familiar', 'seconds', 'planet', 'accidentally', 'realized', 'traditional', 'fighting', 'maria', 'authentic', 'capture', 'pile', 'forgettable', 'compare', 'impressed', 'thoroughly', 'murders', 'wind', 'camp', 'carrying', 'lord', 'killings', 'jobs', 'floor', 'delivers', 'cuts', 'explanation', 'complex', 'consequences', 'grown', 'brain', '12', 'morality', 'entertain', 'smile', 'curious', 'quick', 'angels', 'diamond', 'village', 'source', 'disturbing', 'haunting', 'melodrama', 'extraordinary', 'riding', 'step', 'ring', 'below', 'stevenson', 'adventures', 'nevertheless', 'drunk', 'caused', 'variety', 'growing', 'utterly', 'moved', 'memories', 'bear', 'bugs', 'gang', 'facts', 'aliens', 'paint', 'nazi', 'newman', 'rape', 'formula', 'caprica', 'rangers', 'catholic', 'knowledge', 'latino', 'hugh', 'cheech', 'mass', 'rambo', 'dinosaur', 'hanzo', 'jackman', 'favourite', 'tears', 'kitty', 'foot', 'hanging', 'spoiled', 'clichéd', 'sitcom', 'fortunately', 'native', 'resolution', 'round', 'physical', 'tracy', 'desert', 'fit', 'laurel', 'amongst', 'imagination', 'ironically', 'strongly', 'graphic', 'workers', 'lacking', 'inept', 'million', 'listen', 'minor', 'disguise', 'africans', 'nude', 'combination', 'difference', 'hint', 'universal', 'torture', 'sucked', 'insult', 'gary', 'mission', 'remind', 'product', 'terrific', 'comparison', 'suppose', 'jail', 'possibility', 'asked', 'mr', 'proud', 'desire', 'insight', 'cost', 'flawed', 'combined', 'necessary', 'fail', 'consists', 'suspect', 'focuses', 'cop', 'loses', 'anthony', 'behavior', 'meeting', 'shock', 'committed', 'dig', 'suffers', 'succeeds', 'worthy', 'wedding', 'sat', 'prostitute', 'ellen', 'anna', 'damn', 'enter', 'flicks', '40', 'plant', 'promise', '1980s', 'warm', 'visually', 'satisfying', 'miserably', 'clichés', 'correct', 'endearing', 'opens', 'bet', 'pair', 'independent', 'appropriate', 'tim', 'substance', 'talents', 'aka', 'competent', 'winner', '#', 'teenagers', 'destined', 'drawn', 'results', 'unfortunate', 'delivered', 'relate', 'views', 'gundam', 'contain', 'philosophy', 'visuals', 'craft', 'mixed', 'nasty', 'comical', 'spielberg', 'glory', 'wall', 'fu', 'kick', 'sticks', 'viewed', 'stone', '11', 'training', 'held', 'interviews', 'psychological', 'relation', 'enjoying', 'rogers', 'recognize', 'lily', 'religion', 'chapter', 'unrealistic', 'ride', 'granted', 'handled', 'donald', 'trailers', 'bridge', 'generation', 'mankind', 'americans', 'yawn', 'd', 'rise', 'pity', 'sexually', 'spectacular', 'oil', 'gory', 'ice', 'sword', 'chest', 'topless', 'gags', 'dubbed', 'noticed', 'weapon', 'burned', 'obsessed', 'anne', 'lifetime', 'strength', 'quiet', 'visit', 'rocket', 'discover', 'nutcracker', 'fairy', 'renting', 'occasionally', 'celebrity', 'ultimate', 'treatment', 'choose', 'briefly', 'vietnam', 'favor', 'steals', 'north', 'los', 'remembered', 'core', 'heavily', 'che', 'spanish', 'exciting', 'chong', 'tall', 'flying', 'streep', '`', 'titanic', 'moag', 'gon', 'na', 'sits', 'daily', 'meanwhile', 'circumstances', 'rings', 'clara', 'giant', 'gordon', 'toy', 'tongue', 'invisible', 'endless', 'chief', 'magnificent', 'speech', 'technical', 'damon', 'weapons', 'reminded', 'bothered', 'rolling', 'hip', 'sleep', 'unknown', 'boris', 'karloff', 'holding', 'accident', 'offers', 'stayed', 'harris', 'european', 'graphics', 'hotel', 'environment', 'technically', 'beast', 'mid', 'handsome', 'lovable', 'nurse', 'painfully', 'prequel', 'asking', 'winter', 'description', 'fictional', 'whenever', 'believes', 'wonderfully', 'regular', 'depicted', 'affected', 'duty', 'honor', 'bruce', 'cage', 'thousand', 'twilight', 'required', 'mix', 'holes', 'network', 'mom', 'twisted', 'directs', 'lived', 'conversation', 'roll', 'forces', 'cavemen', 'race', 'keys', 'excited', 'san', 'lie', 'carol', 'seek', 'remotely', 'eating', 'celluloid', 'darkness', 'kim', 'pointed', 'everywhere', 'burke', 'adults', 'treated', 'united', 'europe', 'thoughts', 'passing', 'jessica', 'thomas', 'clean', 'passion', 'suited', 'theory', 'random', 'letter', 'describe', 'complain', 'page', 'eddie', 'india', 'loyalty', 'executed', 'foul', 'drink', 'breaks', 'hidden', 'constructed', 'saving', 'paper', 'drinking', 'backwards', 'comedic', 'vehicle', 'duvall', 'theatre', 'assume', 'gem', 'glenn', 'everyday', 'christina', 'develops', 'pilots', 'battles', 'carry', \"don't\", 'numbers', 'attempted', 'convinced', 'stupidity', 'beach', 'emperor', 'genuine', 'guessing', 'pal', 'doc', 'teens', 'simon', 'tragedy', 'academy', 'develop', 'judge', 'section', 'guard', 'fbi', 'private', 'calvin', 'california', 'crowd', 'shallow', 'nobody', 'test', 'albert', 'moon', 'rain', 'ancient', 'chosen', 'surfing', 'holds', 'deadly', 'defeat', 'elsewhere', 'rented', 'accents', 'included', 'tunes', 't', 'pre', 'whilst', 'australian', 'pop', 'harry', '1950s', 'gross', 'tight', 'claustrophobic', 'foster', 'believing', 'resembles', 'clark', 'actions', 'bomb', 'faithful', '3d', 'rachel', 'irish', 'sympathy', 'culkin', 'london', 'intriguing', 'adapted', 'killers', 'christopher', 'scared', 'cruise', 'retired', 'capable', 'g', 'soccer', 'steps', 'press', 'false', 'mildred', 'build', 'benjamin', 'anti', 'appreciated', 'gray', 'sea', 'harsh', 'jesus', 'buying', 'account', 'campbell', 'peace', 'dire', 'ai', 'luck', 'tommy', 'till', 'europeans', 'explains', 'emma', 'bar', 'shakespeare', 'bound', 'appearing', 'melting', 'gene', 'mitch', 'whites', 'meryl', 'alison', 'matrix', 'weather', 'azumi', 'bey', 'moores', 'raising', 'sayuri', 'chairman', 'bug', 'youth', 'starred', 'wear', 'stops', 'jackie', 'hang', 'regret', 'cliché', 'rights', 'mraovich', 'market', 'ensemble', 'tape', 'billy', 'skin', 'crappy', 'sequels', 'mexican', 'involves', 'eve', 'palace', 'visits', 'incidentally', 'routine', 'cameron', 'technology', 'machines', 'matt', 'quest', 'seeks', 'yesterday', 'inspiring', 'helped', 'sympathetic', 'genuinely', 'struggles', 'overlook', 'join', 'splendid', 'glimpse', 'attacked', 'ruin', 'partly', 'grandfather', 'spirited', 'solely', '0', 'rip', 'warned', 'compelled', 'fat', 'frame', 'protagonist', 'spike', 'guessed', 'teacher', 'ignorance', 'successfully', 'bbc', 'murphy', 'couch', 'emphasis', 'queen', 'kline', 'jeffrey', 'bette', 'cares', 'rose', 'liners', 'president', 'featuring', 'dumb', 'laid', 'month', 'kralik', 'ludicrous', 'jimmy', 'driven', 'loss', 'bright', 'served', 'lips', 'surprises', 'finale', 'improbable', 'specific', 'comparisons', 'prime', 'plots', 'spite', 'sisters', 'babe', 'coal', 'lennon', 'involvement', 'bore', 'cutting', 'primarily', 'rooney', 'smart', 'allows', 'drives', 'todd', 'luckily', 'alright', 'flesh', 'conversations', 'existent', 'highlight', '1990', 'abuse', 'deeply', 'claimed', 'simpson', 'revealing', 'demille', 'argument', 'carries', 'flashback', 'cardboard', 'grant', 'experienced', 'tired', 'snl', 'dick', 'convey', 'parker', 'simplicity', 'frustrated', 'gunga', 'din', 'suck', 'beloved', 'energy', 'humans', 'racist', 'attacks', 'basis', 'explicit', 'novels', 'contemporary', 'draw', 'ominous', 'urban', 'attitude', 'chases', 'guts', 'dahmer', 'lion', 'projects', 'cousin', 'uk', 'productions', 'emotionally', 'category', 'innocence', 'el', 'meaningful', 'handed', 'belief', 'chewbacca', 'poster', 'hopes', 'proved', 'notice', 'plane', 'reunion', 'anymore', 'busy', 'learning', '2004', 'heston', '2001', 'rookie', 'twin', 'complicated', 'purple', 'sons', 'key', 'union', 'stanley', 'performed', 'respective', 'rainy', 'choreography', 'gotten', 'hype', 'fly', 'multi', 'stellar', 'influence', 'jungle', 'saturday', 'troops', 'temple', 'cameos', 'send', 'unintentionally', 'tense', 'remaining', 'embarrassing', 'failures', 'drug', 'freak', '80s', 'deaths', 'downright', 'supposedly', 'repeat', 'washed', 'ages', 'closely', 'reduced', 'intentionally', 'text', 'fill', 'relief', 'individual', 'hunt', 'remote', 'chilling', 'chick', 'ingrid', 'artificial', 'speed', 'locked', '1958', 'scare', 'afternoon', 'idiot', 'east', 'shining', 'btk', 'gina', 'audio', 'edward', 'initial', 'accurate', 'photographed', 'uninteresting', 'magazine', 'matthau', 'scientist', 'martians', 'destruction', 'serbs', 'documentaries', 'denzel', 'attempting', 'kung', 'bodies', 'hearts', 'redemption', 'hole', 'bonus', 'screaming', 'blind', 'shoot', 'football', 'mendes', 'medical', 'confusion', 'curly', 'nicholson', 'developing', 'shirley', 'area', 'rules', 'affair', 'bsg', 'includes', 'dentist', 'nightmare', 'picks', 'razor', 'cagney', 'obligatory', 'downhill', 'miyazaki', 'boyle', 'bonanza', 'whitaker', 'suitors', 'iii', 'grave', 'march', 'learned', 'critics', 'pictures', 'october', 'peckinpah', 'sidney', 'scripted', 'israel', 'dana', 'haunted', 'ninja', 'blanks', 'frodo', 'kazuhiro', 'kharis', 'sajani', 'colony', 'stooges', 'vader', 'nostalgia', 'grim', 'repeated', 'underrated', 'michelle', 'wwii', 'eric', 'yelling', 'conspiracy', 'grow', 'aging', 'awfully', 'active', 'thankfully', 'offensive', 'junk', 'piano', 'ought', 'crafted', 'survives', 'splatter', 'board', 'virtual', 'judging', 'satan', 'santa', 'court', '1974', '\\x85', 'antics', 'doll', 'nope', 'striking', 'contrast', 'laughably', 'ideal', 'relevant', 'lacked', 'slight', 'awe', 'nail', 'noticeable', 'considers', '45', 'hence', 'friendly', 'code', 'appealing', 'cities', 'alas', 'embarrassingly', 'stolen', 'rob', 'hungarian', 'definite', 'messed', 'surface', 'tradition', 'portraying', 'sally', 'celeste', 'wreck', 'awry', 'funnier', 'screenwriter', 'bank', 'jar', 'crude', 'alleged', 'matuschek', 'nine', 'hired', 'letters', 'passed', 'felix', 'abrupt', 'proving', 'broken', 'vivid', 'lucky', 'accomplish', 'mere', 'officer', 'neo', 'searching', 'zone', 'billed', 'countless', 'drawing', 'absence', 'helen', 'contrived', 'justify', 'detective', 'reveals', 'manage', 'confident', 'imagined', 'reaction', 'process', 'brooklyn', 'hood', 'answers', 'switches', 'jazz', 'adrian', 'francisco', 'park', 'spell', 'realizes', 'calling', 'z', 'internet', 'nuclear', 'gruesome', 'seeking', 'scripts', 'attract', 'refreshing', 'astonishing', 'paperhouse', 'experiencing', 'immensely', 'enjoys', 'viewings', 'bargain', 'entertained', 'sum', 'movements', 'absurd', 'structure', 'dated', 'threat', 'classics', 'witty', 'meadows', 'julie', 'worthless', 'centers', 'changing', 'pacino', 'greater', 'gratuitous', 'practically', 'corny', 'matters', 'heroes', 'arms', 'deputy', 'lower', 'breaking', 'ultra', 'angle', 'anita', 'caring', 'inevitable', 'involve', 'principle', 'investigate', 'push', 'recommendation', 'therefore', 'worry', 'shortly', 'mirror', 'burning', 'hearted', '60', 'exotic', 'express', 'horrific', 'memoirs', 'disagree', 'shoes', 'notch', 'react', 'individuals', 'armed', 'sides', 'suit', 'satisfy', 'dramas', 'horrors', 'discussing', 'fffc', 'irrelevant', 'nonsense', 'pseudo', 'uneven', 'tedious', 'lesson', 'revealed', 'disbelief', 'relatively', 'mindless', 'seat', 'beer', 'dawn', 'gypsy', 'ladder', 'traveling', 'hearing', 'steel', 'crimes', 'depressing', 'owners', 'metal', 'nose', 'flaw', 'gandhi', 'destiny', 'shed', 'stylized', 'stunt', 'joan', 'usa', 'strip', 'ford', 'grey', 'experiment', 'morris', 'disc', 'mtv', 'sports', 'hudson', 'solo', 'web', 'soft', 'repetitive', 'solve', 'remarkably', 'festival', 'thirty', 'bitter', 'raise', 'master', 'patient', 'occasional', 'creating', 'cabin', 'judd', 'broadcast', 'delightful', 'flair', 'alice', 'weekend', 'drugs', 'perform', 'shall', 'dornhelm', 'brendan', 'donnison', 'tarantino', 'properly', 'wake', 'nowadays', 'russia', 'hitchhiker', 'segment', 'elderly', 'dorothy', 'ratings', 'mormon', 'placed', 'conventions', 'irritating', 'kidnapped', 'nominated', 'designs', 'degree', 'patrick', 'adopted', 'meat', 'insulting', 'similarly', 'explosions', 'fed', 'seventies', 'mansion', 'jon', 'disconcerting', 'danger', 'hundreds', '1964', '1972', 'melodramatic', 'franco', 'daring', 'wallace', 'reb', 'identical', 'scifi', 'decisions', 'highest', 'selfish', 'failure', 'lincoln', 'health', 'recognizable', 'errors', 'reporter', 'stock', '1973', 'secondly', 'raider', 'tapes', 'adeline', 'enthusiasm', 'proof', 'ms', 'haig', 'research', 'unexpected', 'philip', 'letting', '1960', '1980', 'mcqueen', 'dan', 'performers', 'argue', 'clips', 'worlds', 'apply', 'disease', 'rural', 'narrator', 'precious', 'muslims', 'suffered', 'surrounding', 'woody', 'et', 'report', 'giallo', 'fabulous', 'pet', 'douglas', 'ian', 'dollars', 'costs', 'rooms', 'mountain', 'perdition', 'security', 'goers', 'phillip', 'exposed', 'tiresome', 'countries', 'bucks', 'widmark', 'reference', 'plans', 'blatantly', 'barbara', 'trilogy', 'reactor', 'warriors', 'shu', 'qi', 'bike', 'muertos', 'accepted', 'liz', 'oddly', 'der', 'excitement', 'divided', 'separated', 'failing', 'brilliantly', 'cents', 'smooth', 'uncle', 'misery', 'provoking', 'ewan', 'greatly', 'urge', 'clues', 'bears', 'jonathan', 'bands', 'fights', 'sheila', 'kyle', 'nel', 'mummy', 'navy', 'hag', 'bravo', 'attend', 'grendel', 'beowulf', 'embarrassed', 'troubled', 'reader', 'zhang', 'editor', 'dangerous', 'wolf', 'igor', 'miners', 'coalwood', 'rukh', 'radiation', 'scheme', 'carried', 'comics', 'hazing', 'rises', 'throws', 'sport', 'andré', 'mickey', 'overly', 'pixar', 'kibbutz', 'katsu', 'stereotypical', 'hackenstein', 'offside', 'gillian', 'elvira', 'roddy', 'thieves', 'osa', 'melissa', 'grasshoppers', 'johansson', 'leia', 'lanisha', 'acquired', 'blew', 'consistent', 'table', 'reads', 'newspaper', 'breakfast', 'insert', 'angel', 'faced', 'talks', 'basement', 'hates', 'complaint', 'pays', 'jamie', 'grab', 'loser', 'importance', 'r', 'dee', 'classes', 'static', 'brains', 'boom', 'kinda', 'related', '[', ']', 'st.', 'merry', 'sing', 'magician', 'amusingly', 'prone', 'delivery', 'program', 'guilty', 'passionate', 'reports', 'montage', '21st', '30s', 'staring', 'cheated', 'block', 'predecessors', 'weakest', 'complaints', 'julia', 'humanity', 'sean', 'transformed', 'pulse', 'knife', 'attached', 'holt', 'elephant', 'graveyard', 'shoots', 'wasting', 'largely', 'insane', 'poe', 'casper', 'borrowed', 'boogeyman', 'goofy', 'sounded', 'handful', 'h', 'm', 'improved', 'postcard', 'buster', 'combat', 'intentions', 'players', 'pulp', 'legends', 'romero', '1999', 'stereotypes', 'toilet', 'reluctant', 'cared', 'outcome', 'slapstick', 'center', 'discovering', 'shine', 'sorts', 'fisher', 'corpse', 'superhero', 'unfunny', 'root', 'overcome', 'carlito', 'corner', 'margaret', 'arrives', 'co.', 'opened', 'comeuppance', 'ambitious', 'snow', 'persona', 'c', 'underneath', 'creation', 'cash', 'filmmaker', 'goal', 'integrity', 'interestingly', 'liberal', 'tons', 'westerns', '1948', 'moody', 'specially', 'surreal', 'explaining', 'colour', 'transformation', 'element', 'significant', 'originality', '70s', 'earl', 'wacky', 'siblings', 'possibilities', 'blowing', 'jared', '1976', 'personalities', 'intimate', 'groups', 'undoubtedly', 'objective', 'alternate', 'listening', 'witnessed', 'ensues', 'manhattan', 'dawson', 'kane', 'ear', 'psychic', 'rampage', 'sounding', 'understatement', 'content', 'struck', 'finger', 'charlotte', 'owner', 'marc', 'interpretation', 'types', 'balls', 'idiotic', 'hardcore', 'biased', 'presents', 'jealousy', 'lights', 'mannered', 'performer', 'mrs.', 'accounts', 'wayne', 'pat', 'butt', 'tie', 'spin', 'clothing', 'paintings', 'previously', 'endure', 'whoever', 'captivating', 'monsters', 'honesty', 'rank', 'creatures', 'tanya', 'vengeance', 'amy', 'fingers', 'buddies', 'appropriately', 'technique', 'jet', 'closer', 'priest', 'fodder', 'fallen', 'desired', 'trial', 'baldwin', 'p.s.', 'tear', 'underground', 'civil', 'statements', 'calls', 'adding', 'rush', 'analysis', 'domino', 'causes', 'stages', 'suffering', 'escapes', 'chased', 'chasing', 'incompetent', 'mentally', 'odds', 'painted', 'lam', 'li', 'anytime', 'jerry', 'stealing', 'status', 'pole', 'swimming', 'bon', 'meaningless', 'curiosity', 'i.e.', 'aftermath', 'user', 'void', 'river', 'propaganda', 'captive', 'wondered', 'tortured', 'universe', 'politics', 'spoil', 'unfold', 'mobile', 'heaven', 'forest', 'romp', 'destroy', 'pretentious', 'paced', 'catchy', 'spoken', 'ebert', 'concerns', 'method', 'potter', 'immediate', 'lessons', 'designed', 'afterwards', 'slashers', 'clock', 'flacks', 'higher', 'destroying', 'button', 'sinister', 'disfigured', 'outrageous', 'acid', 'topic', '14', 'jr', 'stumbled', 'collective', 'landscape', 'jules', 'believed', 'smoke', 'fortier', 'superficial', 'roy', 'silver', 'sharp', 'trail', 'disappear', 'threatened', 'management', 'pun', 'dedicated', 'patients', 'portray', 'buff', 'disappoint', 'heights', 'matthew', 'debut', 'vote', 'payne', 'confrontation', 'sensitive', 'limit', 'blair', 'comfortable', 'atrocious', 'damsel', 'grasp', 'hide', 'illiterate', 'someday', 'revolution', 'u', 'shooter', 'iraq', 'malone', 'burn', 'warrior', 'dozens', 'miserable', 'aired', 'natasha', 'flashbacks', 'malcolm', 'blank', 'gripping', 'primary', 'baker', 'introduction', 'threw', 'scientists', 'criminal', 'phantom', 'convicted', 'lawyer', 'unconvincing', 'incarnation', 'behave', 'explosion', 'exist', 'paris', 'string', 'mildly', 'creep', 'trapped', 'checking', 'constant', 'displayed', 'mature', 'carefully', 'titled', 'expedition', 'salt', 'decades', 'stood', 'detailed', 'ran', 'careers', 'creates', 'print', 'crisp', 'sloppy', 'borrow', 'lands', 'expression', 'orders', 'cases', 'grows', 'generic', 'watchable', 'politician', 'secure', 'travel', 'principal', 'frightening', 'dose', 'scares', 'robin', 'secretary', 'awards', 'homosexual', 'dean', 'slaves', 'effectively', 'guards', 'gothic', 'erotic', 'translation', 'holiday', 'hoffman', 'florida', 'barry', 'strictly', 'bart', 'stale', 'whereas', 'ghosts', 'mistakes', 'indians', 'fought', 'led', 'flynn', 'meatball', 'industrial', 'infected', 'kicking', 'presumably', 'pulling', 'shoddy', 'narrated', 'picking', 'tied', 'ruthless', 'don', 'jeremy', 'domestic', 'romanian', 'sin', 'shocked', 'putting', 'france', 'gerard', 'cancer', 'popularity', 'spots', 'russ', 'stiff', 'dialogs', '2006', 'nation', 'service', 'multiple', 'prepared', 'relating', 'sleeping', 'asleep', 'peoples', 'remain', 'receive', 'supernatural', 'numbing', 'convince', 'h.', 'facial', 'prey', 'undead', 'independence', 'avoided', 'kinds', 'uninspired', 'photographer', 'qualities', 'praise', 'stanwyck', 'credibility', 'annoyed', 'blandings', 'muriel', 'loy', 'ho', 'cheaply', 'substitute', 'poetry', 'shades', 'per', 'doors', 'enjoyment', 'witness', 'listed', 'sentimental', 'daniel', 'elegant', 'china', 'waitress', 'needless', 'bullets', 'teeth', 'intense', 'cruel', 'sinatra', 'blow', 'conservative', 'hunting', 'fashioned', 'bo', 'succeeded', 'shouting', 'repeating', 'assault', 'mini', 'austen', 'kelly', 'disney', 'shell', 'acts', 'jumps', 'infamous', 'discovers', 'palma', 'peralta', 'screening', 'bashing', '25', 'covered', 'manga', 'psycho', 'occurred', 'breath', 'duc', 'dressing', 'captures', '98', 'apartments', 'duel', 'beats', 'masters', \"you're\", 'providing', 'part2', 'raised', 'germany', 'ginger', 'circus', 'rocks', 'studying', 'chamberlain', 'inhabitants', 'chair', 'blah', 'visiting', 'samurai', 'mayeda', 'diner', 'associated', 'ogre', 'ireland', 'term', 'weaker', 'saxon', 'reached', 'nasa', 'merit', 'opposed', '50s', 'courage', 'mcgregor', 'protée', 'suits', 'frosty', 'esther', 'edith', 'edie', 'yanos', 'fiery', 'halfway', 'commercial', 'hickam', 'rockets', 'introduces', 'journalist', 'importantly', 'dancy', 'alienate', 'pegg', 'blaise', 'misguided', 'summary', 'shadow', 'notable', 'noam', 'ashwar', 'tin', 'manipulated', 'reminds', 'bassenger', 'trek', 'valerie', 'troma', 'richardson', 'battlestar', 'amazed', 'lindy', 'leachman', 'data', 'tara', 'caan', 'ruby', 'laboratory', 'restored', 'darryl', 'aaron', 'iran', 'tolkien', 'fatal', 't.k.', 'sivan', 'sixteen', 'cécile', 'frost', 'johnsons', 'genova', 'partition', 'jedi', 'yoda', 'bless', 'veteran', 'promiscuous', 'donna', 'naive', 'oversexed', 'purely', 'hook', 'visconti', 'arrested', 'warped', 'synopsis', 'sleazy', 'miscast', 'mason', '60s', 'sunday', 'pleased', 'intact', 'realised', 'internal', 'campus', 'university', 'references', 'ah', 'hunted', 'suspenseful', 'obscure', 'garish', 'resemble', 'lonely', 'wrapped', 'trio', 'causing', 'juvenile', 'entering', 'godard', 'ruins', 'mistress', 'scientific', 'stirring', 'sir', 'futuristic', 'exists', 'quietly', 'latest', 'chain', 'silence', 'devoid', 'lions', 'crocodile', 'larger', 'costume', 'ears', 'wears', 'bikini', 'mountains', 'nearby', 'thick', 'stan', 'raven', 's.', 'edmond', 'controlled', 'billing', 'techniques', 'predecessor', 'discussion', 'eats', 'presentation', 'entry', 'mel', 'skirts', 'darker', 'cinematographer', 'ad', 'studies', 'dollar', 'burt', 'distributed', 'beliefs', 'legendary', 'ridiculously', 'indie', 'saves', 'employed', 'losing', 'entered', 'crisis', 'arrival', 'ego', 'croc', 'skit', 'robs', 'slap', 'reputation', '99', 'jay', 'forgets', 'spending', 'protagonists', 'cigarette', 'souls', 'minds', 'base', 'morgan', 'sidekick', 'joseph', 'subsequent', 'manager', 'dinner', 'closing', 'jealous', 'breakdown', 'lubitsch', 'hung', 'expense', 'merits', 'allowing', 'spectacle', 'conscience', 'betrayal', 'recorded', 'strikes', 'coincidences', 'confront', 'rod', 'deserted', 'gender', 'drag', 'appalling', 'plague', 'retirement', 'suspicious', 'construction', 'ambition', 'dreadfully', 'innovative', 'appearances', 'dysfunctional', 'mystical', '1956', 'convertible', 'turner', 'warner', 'acclaimed', 'temper', 'continue', 'agrees', 'othello', 'colman', 'winters', 'approached', 'mccartney', 'broke', 'storytelling', 'priceless', 'geniuses', 'realm', 'jonestown', 'profound', 'viva', 'videos', 'hackneyed', 'indulgent', 'format', 'max', 'cell', 'gallery', 'bearing', 'revelation', 'gradually', 'taxi', 'commit', 'stated', '1993', 'resembling', 'schlock', 'insipid', 'bold', 'basinger', 'guarantee', 'gangs', 'drawings', 'wishes', 'cherish', 'glass', '8th', 'fest', 'outfits', 'b.', 'crusade', 'alcohol', 'dress', 'pointing', 'victorian', 'thief', 'education', 'feminist', 'hiring', 'intellectual', 'wrenching', 'dry', 'ease', 'phelps', 'fired', 'wealth', 'luxury', 'ferrell', 'fault', 'host', 'dragged', 'boeing', 'warren', 'beatty', 'achieve', 'disturbed', 'pertinent', 'denial', 'lay', 'spends', 'motivated', 'imagery', 'exceptionally', 'thrilling', 'pie', 'liking', '19th', 'garris', 'neck', 'target', 'injured', 'lip', 'attraction', 'motives', 'menacing', 'vicious', 'ron', 'abilities', 'cheek', 'jarring', 'sentiment', 'uneducated', 'educated', 'hopkins', 'plaything', 'swedish', 'frontal', 'showcase', 'harvey', 'convoluted', 'fatty', 'psychiatrist', 'surgery', 'wonders', 'backs', 'ringo', 'watches', 'diane', 'raines', 'jolie', 'standout', 'thirteen', 'morbid', 'implication', 'realities', 'extended', 'pivotal', 'population', 'experimental', 'bernie', 'scale', 'fleet', 'realistically', 'anime', 'spelled', 'contact', 'routines', 'mild', 'flow', 'floating', 'disjointed', 'morse', 'penn', 'garner', 'orlando', 'seduce', 'garden', 'locale', 'breathing', 'thrills', 'paranoia', 'reasonable', 'informed', 'indiana', 'idiots', 'pants', 'noble', 'stretch', 'legs', 'joker', 'marty', 'abandoned', '13th', 'stress', 'deanna', 'bullying', 'marie', 'brutally', 'size', 'sammy', '1933', 'drew', 'dwelling', 'engine', 'expressions', 'chaos', 'miraculously', 'separate', 'similarities', 'devine', 'nolan', 'pioneers', 'kicks', 'heels', 'museum', 'figures', 'blames', 'dislike', 'legal', 'satire', 'dogma', 'poke', 'mcconaughey', 'doyle', 'visited', 'assigned', 'installment', '2007', 'del', 'writes', 'susan', 'charlie', 'suggests', 'traffic', 'programs', 'spiritual', 'southern', 'seldom', 'racial', 'skeptical', 'connections', 'lush', 'item', 'fonda', 'unable', 'worried', 'buildings', 'charge', 'steam', 'pitch', 'unrelated', 'concert', 'expert', 'premiere', 'rotten', 'mentions', 'buddy', 'barrel', 'bleak', 'arrogant', 'slimy', 'gathering', 'morals', 'lightweight', 'teach', 'meg', 'forgive', 'replay', 'wanna', 'study', 'puppy', 'firing', 'suspects', 'parody', 'maintain', 'corruption', 'burst', 'dealt', 'differences', 'nelson', 'stronger', 'enigmatic', 'rambling', 'alike', 'e', 'hong', 'kong', 'uncredited', 'carter', 'voyna', 'mir', '1968', 'screened', 'russians', 'epics', 'drawbacks', 'joined', 'losers', 'fits', 'justified', 'sophisticated', 'ridden', 'schaech', 'eastwood', 'salvage', 'anthology', 'outing', 'cringe', 'lively', 'raft', 'inane', 'ugh', 'mysteriously', 'ironic', 'occasion', 'exercise', 'conceived', 'intensity', 'contrary', 'weaves', 'bent', 'hopeless', 'excessive', 'hk', 'pushes', 'wave', 'fireworks', 'polish', 'fortune', 'identities', 'cleverly', 'steady', 'examples', 'robot', 'farm', 'henderson', 'promptly', 'owe', 'primitive', 'zoe', 'safe', 'breathtaking', 'producing', '1959', 'capote', 'organs', '20th', 'engage', 'pitt', 'chills', 'fourth', 'atmospheric', 'tracks', 'partner', 'forcing', 'shy', 'plight', 'skills', 'astronaut', 'settle', 'screenwriters', 'monkeys', 'tank', 'gas', 'louque', 'cambodia', 'gain', 'unrecognizable', 'classy', 'ranks', 'sturges', 'sappy', 'bush', 'grinch', 'con', 'promised', 'desperation', 'vincent', 'radar', 'saddles', 'controversial', 'vulgar', 'replaced', 'confederate', 'dancer', 'ha', 'continued', 'instantly', 'depiction', 'backdrop', 'flawless', 'benefit', 'staged', 'accomplished', 'oliver', 'lyrics', 'broadway', 'blues', 'houses', 'cup', 'overdone', 'automatic', 'explained', 'tessari', 'wire', 'qualifies', 'sudden', 'families', 'godfather', '1960s', 'occurs', 'beings', 'bare', 'ignorant', 'truthful', 'forwards', 'shared', 'blunt', 'polite', 'cruelty', 'dogs', 'choking', 'lollobrigida', 'stunts', 'charismatic', 'tulipe', 'masses', 'atlantis', 'empire', 'j.', 'commander', 'lloyd', 'aged', 'walter', 'physically', 'foolish', 'flop', 'critical', 'reagan', 'rapidly', 'pretends', 'inform', 'freddy', 'k', 'celebrated', 'balanchine', 'olivier', 'mouse', 'brad', 'leslie', 'nomination', 'staying', 'drop', 'aunt', 'prisoner', 'adaption', 'globe', 'rubber', 'display', 'encounters', 'conviction', 'yugoslavia', 'throwing', 'nazis', 'georgia', 'strangely', 'absurdity', 'nineties', 'inspector', 'matched', 'abusive', 'purchase', 'grin', 'voices', 'negatives', 'suave', 'path', 'typically', 'teaching', 'taught', 'dropped', 'decision', 'handle', 'executives', 'parnell', 'regarding', 'thread', 'wide', 'aforementioned', 'elevator', 'recognition', 'underdeveloped', 'headed', 'terrain', 'singer', 'chime', 'earned', 'chainsaw', 'grainy', 'unfolds', 'subtext', 'guilt', 'revolves', 'craig', 'loulou', 'berlin', 'hitler', 'secrets', 'returning', 'vets', 'vet', 'afford', 'tour', 'laura', 'encounter', 'natives', 'dreadful', 'crotch', 'wished', 'refined', '1936', 'clue', 'grotesque', 'nights', 'vice', 'ambiance', 'attracted', 'hairy', 'document', 'terrorist', 'darn', 'tune', 'eternity', 'invincible', 'corporate', 'streets', 'agents', 'microfiche', 'communist', 'unpleasant', 'gulfax', 'bigger', 'evans', 'fosters', 'rot', 'showdown', 'spades', 'repeatedly', 'pia', 'landscapes', 'suitable', 'lucy', 'samantha', 'orphanage', 'digital', 'harmony', 'aiming', 'jovi', 'vampiress', 'snake', 'beating', 'temptation', 'perception', 'ruth', 'label', 'beverly', 'hills', 'alan', 'cheating', 'wing', 'witnesses', 'keith', 'pleasures', 'sticking', 'clinton', 'jews', 'paying', 'limits', 'covers', 'covering', 'walls', 'renaissance', 'glaring', 'punch', 'disappeared', 'scores', 'colin', 'hoped', 'transfixed', 'hides', 'columns', 'cook', 'atrocity', 'pretense', 'bollywood', 'remarks', 'territory', 'pedro', 'clint', 'rebel', 'website', 'alongside', 'planning', 'sadistic', 'wretched', 'indication', 'darren', 'stinker', 'sold', 'corrupt', 'precisely', 'educational', 'shares', 'improve', 'lena', 'dub', 'gracie', '3rd', 'mental', 'directions', 'criticize', 'royal', 'katie', 'frequently', 'swept', 'waited', 'sirk', '1963', 'walks', 'refuse', 'vs.', 'eerie', 'scariest', 'enormous', 'affect', 'wardrobe', 'geico', 'lesbian', 'pernell', 'gunfights', 'hundred', 'dutch', 'calm', 'kay', 'fart', 'mates', 'excruciatingly', 'immature', 'sho', 'youthful', 'evidently', 'catalog', 'responsibility', 'rednecks', 'derek', 'ingredients', 'timeless', 'dare', 'omar', 'hop', 'poem', 'crown', 'innovation', 'wings', 'randomly', 'fifty', 'rick', 'breasts', 'likewise', 'ricci', 'sadness', 'returned', 'yimou', 'operation', 'smmf', 'quentin', 'dragon', 'punches', 'stir', 'lighthearted', 'snowman', 'choreographed', 'abysmal', 'minded', 'pitiful', 'fare', 'adequate', 'olga', 'mines', 'models', 'composed', 'frequent', 'tooth', 'directly', 'hat', 'sherry', 'murderer', 'touched', 'sunrise', 'supported', 'morgue', 'oates', 'saga', 'preferred', 'blooded', 'implausible', 'distant', 'violated', 'interview', 'coaster', 'equivalent', 'barney', 'claymation', 'meteor', 'egg', 'downbeat', 'wolves', 'foxes', 'bloom', 'heroic', 'lucienne', 'dozen', 'exceptions', 'habit', 'choices', 'vixen', 'gavin', 'meyer', 'credible', 'furthermore', 'fist', 'virtually', 'improvised', 'gummer', 'discovery', 'vaudeville', 'senile', 'sarah', ':)', 'asylum', 'hawke', 'determine', 'tulip', 'forth', 'keller', 'imitation', 'tremors', 'predict', 'saint', 'maher', 'capitalism', 'astro', 'irony', 'ueto', 'mockumentary', 'studs', 'gable', 'professor', '1930', 'bakshi', 'noll', 'region', 'nino', 'vicky', 'portrays', 'argonne', 'oak', 'ridge', 'corporation', 'league', 'royston', 'vasey', 'paquin', 'pg', 'satanic', 'stiller', 'hindus', 'flik', 'esp', 'sheeta', 'pains', 'lou', 'ghetto', 'hellborn', 'puerto', 'drummond', 'ifans', 'ebts', 'hazel', 'phil', 'darling', 'hmmm', 'rushes', 'neighbor', 'trees', 'circle', 'weirdo', 'rude', 'leo', 'peaceful', 'hysterical', 'cain', 'resulting', '1981', 'sensual', 'remakes', 'petty', 'innocently', 'shelf', 'bait', 'promote', 'assembled', 'posing', 'fields', 'transition', 'priests', 'profanity', 'rub', 'virus', 'demon', 'fx', 'marines', 'maker', 'apes', 'dimension', 'proceedings', 'claus', 'wackiness', 'reel', 'acquire', 'wizard', 'gained', 'excess', 'shenanigans', 'projected', 'counts', 'bay', 'nerves', 'walked', 'everytown', 'attacking', 'titles', 'redeemed', 'raymond', 'concludes', 'progress', 'severely', 'proudly', 'experts', 'amanda', 'peet', 'supremacy', 'tops', 'spotlight', 'chalk', 'greengrass', 'electric', 'generate', 'pierce', 'bean', 'regrets', 'resources', 'drags', 'ape', '1932', 'varied', 'tribes', 'forty', 'jaws', 'ivory', 'deathbed', 'maureen', 'mustache', 'tyler', 'marilyn', 'anger', 'rewarded', 'planned', 'collaboration', 'quarters', 'vollins', 'aid', 'edgar', 'recreate', 'mall', 'visible', 'blatant', 'stereotype', 'coleman', 'ditched', 'international', 'espionage', 'budapest', 'canadian', 'c-', 'inventive', 'heaton', 'parable', 'evident', 'betsy', 'notes', 'maturity', 'admiration', 'interactions', 'marginally', 'anderson', 'unaware', '1950', 'hilarity', 'forties', 'niece', 'craven', 'deaf', 'barnes', 'rid', 'delight', 'carrie', 'spoof', 'beg', 'resulted', 'sank', 'participating', 'porno', 'guzman', '1975', 'scenarios', 'contained', 'warming', 'clash', 'sell', 'bickering', 'pimp', 'du', 'righteous', 'promoted', 'position', 'hungary', 'realises', 'stormy', 'insults', 'nervous', '22', 'famously', 'dreamy', 'kudos', 'achievement', 'concentrate', 'bio', 'posters', 'b5', 'vaguely', 'greed', 'nicolas', 'hopper', 'correctly', 'labeled', 'timing', 'basketball', 'approaching', 'earnest', 'manipulative', 'seth', 'christine', 'cinderella', 'murderous', \"o'brien\", 'whit', 'pleasantly', 'lindsay', 'consequently', 'beatles', 'informative', 'counterparts', 'n', 'masterfully', 'requisite', 'dandy', 'defined', 'sincere', 'destructive', 'via', 'tackling', 'gecko', 'ripped', 'aimed', 'ta', 'hum', 'screwed', 'contractor', 'unhappy', 'entice', 'buscemi', 'kiss', 'rosario', 'coaxes', 'hurting', 'packs', 'insists', 'aim', 'mullet', 'goodbye', 'technological', 'advances', '500', 'clueless', 'housewife', 'dropping', 'lapses', 'jerk', 'toys', 'thrillers', 'helpless', 'reviewer', 'manipulating', 'affecting', 'hans', 'zimmer', 'elliot', 'brilliance', 'daisy', 'graves', '1922', 'recklessness', 'meighan', 'goodness', 'woeful', 'arm', 'chewing', 'excellence', 'attained', 'setup', 'schemes', 'shearer', 'cary', 'ward', 'gigolo', 'fiancée', 'bathroom', 'knock', 'essence', 'sponge', 'voyager', 'contract', 'janet', 'leigh', 'duds', 'pushing', 'buttons', 'signed', 'girlfriends', 'slept', 'guest', 'difficulty', 'triangle', 'casts', 'airport', 'madonna', 'nacho', 'smiling', 'afterward', 'ohio', 'nuances', 'ness', 'hindsight', 'powerfully', 'terrorism', 'goldblum', 'tendency', 'broad', 'stahl', 'finch', 'legion', 'camaraderie', 'sacrifice', 'sleepwalkers', 'carnage', 'hungry', 'amick', 'crush', 'adorable', 'bites', 'welcomed', 'stabs', 'bone', 'brady', 'crippled', 'dim', 'clive', 'barker', 'tobe', 'deranged', 'lick', 'demented', 'mouthed', 'neglect', 'gossip', 'celebration', 'undertext', 'prostitutes', 'object', 'backgrounds', 'cultural', 'jennifer', 'unlikable', 'un', 'horny', 'exploit', 'sleaze', 'undertones', 'tame', 'benefited', 'ya', 'abigail', 'greg', 'baptist', 'atheist', 'timothy', 'arbuckle', 'guaranteed', 'staff', 'paradise', 'shi', 'sue', 'loaded', 'token', 'nonetheless', 'requires', 'resorting', 'molly', 'autobiographical', '1949', 'ticket', 'exploring', 'deeper', 'medium', 'technicolor', 'preached', 'session', 'uncomfortable', 'thru', 'ourselves', 'zeon', 'secretly', 'pocket', 'sweeping', 'tragedies', 'devastating', 'conveys', 'closest', 'affects', 'proportions', 'guitar', 'freshman', 'rave', 'hallway', 'coupled', 'tea', 'mortensen', 'crawl', 'bronson', 'tricks', 'tango', 'learnt', 'suspension', 'yellow', 'bath', 'incidental', 'violin', 'convention', 'lucas', 'establishing', 'shoulder', 'bronze', 'accompanied', 'brave', 'panic', 'apologize', 'nerd', 'sporting', 'occur', 'switching', 'slaughter', 'factor', 'traumatic', 'alicia', 'christensen', 'balance', 'columbia', 'deathly', 'decidedly', 'slaughterhouse', 'obnoxious', '2009', 'bulk', 'seats', 'global', 'trials', 'slamming', 'trace', 'gaming', 'wtc', 'upper', 'casualty', 'risk', 'profiler', 'evaluate', 'prefer', 'cookie', 'canyon', 'pamela', 'collar', 'knockout', 'munster', 'wax', 'tritter', 'gruff', 'vindictiveness', 'authority', 'backdrops', 'skins', 'wesley', 'boothe', 'widow', 'remarkable', 'describes', 'leap', 'illness', 'organization', 'distress', 'laws', 'votes', 'inspirational', 'horses', 'shopping', 'verge', 'fragile', 'relaxed', 'furious', 'factory', '90s', 'pool', 'awhile', 'logo', 'copies', 'musician', 'tad', 'quotable', 'apollonia', 'stadium', 'cherry', 'featurette', 'stack', 'bacall', 'chops', 'hadley', 'emerging', 'widowed', 'bombs', 'dvds', 'homage', 'deemed', 'unimaginative', 'dubbing', 'italy', 'incident', 'brandon', 'pops', 'prisoners', 'coca', 'marching', 'flashing', 'begun', 'breast', 'foxx', 'comedian', 'notably', 'biz', 'cyborg', 'populated', 'demonic', 'blond', '18', 'masterpieces', 'deepest', 'sand', 'athletic', 'sharply', 'russell', 'morally', 'robbed', 'buzz', 'amateurs', 'irritated', '2nd', 'thailand', 'yankee', 'hank', 'beckinsale', 'walker', 'citizens', 'remembering', 'spoiling', 'narration', 'evokes', 'vividly', 'steer', 'foreigners', 'trashy', 'gornick', 'unbearably', 'dialogues', 'messages', 'glowing', 'speeches', 'avenger', 'platform', 'gooey', 'pressed', 'wealthy', 'inferior', 'characterization', 'andrew', 'thumbs', 'struggling', 'pushed', 'produce', 'professors', 'tap', 'ant', 'lds', 'smoked', 'punk', 'drunken', 'jerks', 'belong', 'readers', 'wholly', 'rapid', 'origins', 'e.g.', 'fever', 'flag', 'esque', 'sole', 'survivor', 'noises', 'climatic', 'neglected', 'sorcery', 'rampant', 'retarded', 'd-', 'sh!t', 'inventor', 'fluff', 'rendition', 'phenomenal', 'abound', 'photo', 'drums', 'insultingly', 'circles', 'bible', 'instance', 'complaining', 'developments', 'hints', 'gate', 'mercifully', 'silliness', 'truck', 'reactions', 'occasions', 'voiced', 'inspiration', 'shortcomings', 'virtuous', 'vonnegut', 'harrison', 'inadvertently', 'frozen', 'supportive', 'kansas', 'convicts', 'senses', 'blake', 'punishment', 'load', 'rejected', 'assistant', 'useful', 'observed', 'elliott', 'cape', 'terrifying', 'ties', 'harder', 'upset', 'burton', 'chimp', 'upside', 'threats', 'trained', 'concerning', 'switch', 'unbelievably', 'hypnotized', 'mabuse', 'testament', 'regain', 'compound', 'greyson', '1957', 'revolt', 'currently', 'bald', 'preston', 'healthy', 'christians', 'cu', 'creativity', 'mike', 'pursuit', 'hustler', 'dustin', 'survival', 'symbolic', 'sunshine', 'midnight', 'poignant', 'quote', 'mia', 'billions', 'notorious', 'blazing', 'carbon', 'von', 'assumed', 'edition', 'suspend', 'greedy', 'ashley', 'theaters', 'scrooge', 'stopped', 'buffalo', 'selling', 'porter', 'april', 'pardon', 'occupied', 'creator', 'textbook', 'duo', 'succeed', 'satisfied', 'subplot', 'outlandish', 'weaponry', 'mode', 'lit', 'rely', 'driveway', 'property', 'function', 'newport', 'abruptly', 'delon', 'pray', 'gunman', 'yarn', 'directorial', 'automatically', 'minimum', 'stomach', 'mac', 'shaped', 'kik', 'frustration', 'kiki', 'firstly', 'connect', 'announced', 'hometown', 'happily', 'philipe', 'farmer', 'hay', 'recruits', 'stepping', 'truffaut', 'admired', 'sings', 'previews', 'shrek', 'natured', 'rourke', 'dave', 'conditions', 'dreyfuss', 'incorrect', 'filler', 'draws', 'onstage', 'macaulay', 'discreet', 'distracting', '1986', 'cap', 'bert', 'baryshnikov', 'sofa', '---', 'hears', 'cole', 'sending', 'abc', 'lasted', 'favorites', 'jansen', 'delicate', 'bars', 'jails', 'damme', 'oldest', 'loads', 'framing', 'exploits', 'stopping', 'bullet', 'expects', 'coherent', 'g.', '1900', '1920s', 'noted', 'landmark', 'victory', 'curate', 'cheerful', 'increasingly', 'distorted', 'radical', 'represented', 'professionals', 'genocide', 'slovenians', 'slovenia', 'yna', 'outraged', 'accusations', 'vukovar', 'committing', 'ethnic', 'misleading', 'bosnian', 'borders', 'pro', 'sought', 'gut', 'respects', 'resolved', 'undeveloped', 'slack', '17', 'wet', 'swords', 'rider', 'thaw', 'robinson', 'deceased', 'fletcher', 'connected', 'minimal', 'lisa', 'stylish', 'trademarks', 'females', 'intention', 'asset', 'illogical', 'recognized', 'benny', 'myrna', 'marshall', 'connecticut', 'supply', 'grip', 'ham', 'distinct', 'hinted', 'bunker', 'ammo', 'varying', 'silently', 'faster', 'sickness', 'defend', 'passes', 'translated', 'continuously', 'gaps', 'brando', 'shaky', 'bones', 'interludes', 'padding', 'sympathize', 'hilariously', 'summed', 'gangsters', 'miller', 'crossing', 'criticism', 'sullivan', 'colored', 'reflect', 'doubts', 'nolte', 'specifically', 'persons', 'despair', 'compares', 'tacky', 'delves', 'sync', 'mentality', 'regardless', 'healing', 'official', 'deserving', 'abducted', 'kidnappers', 'inhabited', 'sacrifices', 'ransom', 'fog', 'bondage', 'pale', 'impudent', 'steiner', 'francis', 'induce', 'slut', 'lure', 'phillips', 'realizing', 'outdated', 'fiend', 'commentator', 'continent', 'nymphomaniac', 'holmes', 'ball', '1979', 'stalking', 'hers', 'perspectives', 'ragged', 'edges', 'illustrate', 'cops', 'department', 'kubrick', 'ingenious', 'beneath', 'sparse', 'mocking', 'agency', 'provoke', 'lately', 'chavez', 'manipulate', 'peters', 'maggie', 'understandable', 'subjected', 'prospect', 'illustrious', 'glued', 'anticipated', 'boils', 'youtube', 'pearl', 'melt', 'rough', 'germs', 'malaria', 'eager', 'marxist', 'decapitated', 'chan', 'remove', 'emerge', 'elementary', 'explosive', 'reward', 'leather', 'transparent', 'zoey', 'slim', 'farther', 'obsessive', 'rage', 'catholics', 'helicopter', 'stranger', 'blade', 'inappropriate', 'stacy', 'county', 'captivated', 'thoughtful', 'om', 'portions', 'topics', 'precise', 'pathos', 'lane', 'mount', 'storm', 'assassination', 'servant', 'cable', 'farce', 'switzerland', 'recording', 'doomed', 'nuns', 'bacon', 'moronic', 'owen', 'gladys', 'griffith', 'patience', 'swear', 'leaning', 'refers', 'shook', 'relic', 'misconceptions', 'inevitably', 'representation', 'episodic', 'helpful', 'telescope', 'illustrated', 'summarize', 'plastic', 'bedroom', 'releases', 'edgy', 'examined', 'lawrence', 'yuma', 'alternately', 'replacement', 'adams', 'misses', 'credited', 'equipment', 'storyboard', '1940s', 'ollie', 'parallel', 'rear', 'noise', 'casino', 'michell', 'reid', 'rosie', 'che(2008', 'task', 'challenge', 'stratton', 'australia', 'peasants', 'part1', 'outlook', 'deltoro', 'excruciating', 'latinos', 'societies', 'booth', 'weakness', 'mechanics', 'unforgettable', 'quaint', 'swell', 'beek', 'literal', 'funhouse', 'ocean', 'lili', '1930s', 'dictator', 'versa', 'household', 'exceptional', 'friendships', 'snap', 'glenda', 'displays', 'christie', 'smaller', 'patsy', 'filling', 'misunderstandings', 'judgmental', 'fitting', 'rounding', 'rats', 'bumbling', 'crain', 'companion', 'drift', 'sprinkled', 'aborigines', 'oppressive', 'practice', 'huh', 'amazingly', 'lara', 'col', 'lends', 'stinks', 'association', 'hiding', 'faust', 'blocker', 'hoss', 'patriarch', 'unfair', 'conductor', 'devoted', 'comforting', 'naval', 'explanations', 'hostel', 'contextual', 'referred', 'hotter', 'speaks', 'feminine', 'sentences', 'highlights', 'semblance', 'stacking', 'wildly', 'kisses', '=', 'deliverance', 'rhys', 'eastern', 'acceptance', 'roommates', 'accessible', 'moonstruck', 'apology', 'ira', 'protestant', 'disgust', 'cards', 'denying', 'disappears', 'voyage', 'principals', 'bravado', 'relations', 'tribute', 'vacation', 'plantation', 'christ', 'tuned', 'suggestion', 'dash', 'split', 'wore', 'issue', 'brand', 'originally', 'recover', 'heat', 'dances', 'deck', 'horrendous', 'demme', 'mst3', 'linked', 'driver', 'mae', 'arcane', 'filmmaking', 'article', '1990s', 'contemplating', 'drops', '1.5hrs', 'cuba', 'nephew', 'heartfelt', 'reese', 'rightly', 'superman', 'conveying', 'range', 'asian', 'subpar', 'crack', 'menace', 'achieved', 'knightly', 'guinea', 'pig', 'receives', 'testing', 'cafeteria', 'thoughtlessness', 'colonialism', '35', 'segments', 'shape', 'beales', 'exploitative', 'condition', 'criterion', 'hire', 'pursued', 'werewolf', 'stumbles', 'preposterous', 'neatly', 'challenges', 'codes', 'civilization', 'motions', \"o'dell\", 'replies', 'nerdy', 'riley', 'encourages', 'attenborough', 'penelope', 'sandrich', 'depression', 'sailor', 'bake', 'enjoyably', 'brunette', 'meteorite', 'drake', 'colbert', 'vanilla', 'comprised', 'enlightened', 'mariner', 'angeles', 'prohibition', 'diplomat', 'undermine', 'trade', 'upcoming', 'patterns', 'peak', 'bridges', 'dude', 'cringeworthy', 'pullman', 'hailed', 'enterprise', 'riveting', 'travesty', 'hines', 'sellers', 'screw', 'millionaire', 'buffs', 'coma', 'citizen', 'countryside', 'mushrooms', 'shake', 'admirable', 'males', 'rockwell', 'tidy', 'ish', 'mournful', 'penguins', 'titular', 'indulge', 'disturb', 'stabbing', 'wes', 'della', 'miracle', \"'em\", 'reaching', 'assassin', 'ashamed', 'chronicles', '1971', 'qualified', 'understood', 'harold', 'priyadarshan', 'briggs', 'pack', 'yours', 'vain', 'dish', 'thugs', 'cartoons', 'stumbling', 'crying', 'lowe', 'arnie', 'aquatic', 'oklahoma', 'wakes', 'rebar', 'burr', 'debenning', 'boobs', 'coast', 'louise', 'cries', 'affection', 'expose', 'q', 'gettaway', 'winstone', 'jeepers', 'creepers', 'ads', 'generous', 'telly', '1996', 'distance', 'walken', 'neat', 'hating', 'dolls', 'triumph', 'actuality', 'hallan', 'practical', 'koltai', 'sentimentality', 'mothers', 'eileen', 'atkins', 'woven', 'helmer', 'grudge', 'balcony', 'gellar', 'asia', 'hokey', 'nagra', 'reloaded', 'thereof', 'heartbreaking', 'ethan', 'cloris', 'valuable', 'surprising', 'sibrel', 'undermines', 'combine', 'romances', 'barrymore', 'cartouche', 'clair', 'comparable', 'threatening', 'fitzgerald', 'faults', 'nicholas', 'lopez', 'significantly', 'fling', 'thunderbird', 'sore', 'hunters', 'stein', 'patriotic', 'demands', 'worrying', 'powell', 'pregnant', 'grass', 'elite', 'locken', 'brotherhood', 'sterling', 'clumsy', 'arrive', 'auction', 'adoption', 'rooted', 'sins', 'boredom', 'sparks', 'gosha', 'pride', 'breed', 'alarm', 'heist', 'lumet', 'marisa', 'spells', 'dumber', 'goyôkiba', 'harlow', 'digger', 'climb', 'seduced', 'upbringing', 'frightened', 'declare', 'spice', 'millions', 'defense', 'sneak', 'pen', 'preparing', 'commando', 'accepts', 'decline', 'explores', 'sesame', 'useless', 'threads', 'frankie', 'hamilton', 'surfer', 'kicked', 'cajuns', 'capacity', 'rains', 'presume', 'bowl', 'daddy', 'sollett', 'tighter', 'parade', 'cusack', 'duff', 'landed', 'tastes', 'moms', 'dagger', 'kitten', 'chicago', 'zinn', 'archives', 'tubbs', 'portuguese', 'compliment', 'safari', 'e.', 'exaggeration', 'wayan', 'graham', 'conventional', 'congorilla', 'yada', 'kolchack', 'mariscal', 'infierno', 'baron', 'vs', 'mameha', 'patron', 'shaved', 'pakistan', 'prevalent', 'closed', 'scoop', 'filmography', 'spatial', 'thematics', 'lift', 'shrine', 'lupino', 'mandy', 'r2', 'c3po', 'chewie', 'gather', 'texans', 'pazu', 'rumors', 'fuzzy', 'rehearsal', 'penguin', 'clerics', 'fontaine', 'gambling', 'hodiak', 'pintilie', 'algy', 'fiennes', 'monstervision', 'prices', 'rates', 'britain', 'wins', 'tyrannical', 'skinny', 'placing', 'kindly', 'ritual', 'disco', 'kelso', 'cheerleader', 'foreigner', 'joking', 'noticing', 'bite', 'prospective', 'calamai', 'girotti', 'sensibilities', 'anyhow', 'definitive', 'praised', 'dime', 'file', 'angered', 'exceedingly', 'package', 'gabel', 'rack', 'irresponsible', 'nakedness', 'purposely', 'bolton', 'fuss', 'schmaltzy', 'constitutional', '79', 'vcr', 'maxwell', 'scoggins', 'cinemax', 'spaceship', 'intruder', 'ariel', 'fantasies', 'padded', 'innuendo', 'muster', 'bang', 'cardona', '1969', '1977', 'murray', 'virtue', 'reds', 'essential', 'nutshell', 'celestial', 'accompanying', 'famed', 'hopping', 'mumbling', 'gibberish', 'render', 'anachronistic', 'dracula', 'pad', 'horned', 'mischief', 'exiting', 'hmm', 'strokes', 'contempt', 'outsiders', 'planes', 'height', 'prediction', 'marred', 'expressionist', 'seductive', 'barbarism', 'elevated', 'aircraft', 'massey', 'expressed', 'dragnet', 'commitment', 'ingenuity', 'fronts', 'assassins', 'psyched', 'link', 'authorities', 'boggling', '150', 'robots', 'headquarters', 'camcorder', 'stiles', 'signature', 'reserve', 'interpol', 'poignancy', 'longing', 'brosnan', 'screams', 'smallest', 'forge', 'tempted', 'karma', 'intercourse', 'spins', 'elephants', 'distinction', 'fellows', 'underwater', 'flickering', 'glimpses', 'weismuller', 'swim', 'loincloth', 'bats', 'vendetta', 'whiny', 'ski', 'guru', 'engaged', 'samuel', 'enlists', 'bateman', 'shifts', 'shift', 'ranked', 'inexplicably', 'fanatic', 'cried', 'careless', 'mish', 'mash', 'pasting', 'labor', 'swearing', 'donuts', 'legitimate', 'harlequin', 'canada', 'ummmph', 'proclaim', 'ps', 'foreground', 'adjust', 'myth', 'flatly', 'basics', 'inconsistent', 'kramer', 'cliff', 'flushed', 'mysticism', 'astonishingly', 'dignity', 'purposes', 'margo', 'happiness', 'vulnerable', 'misadventures', 'cori', 'ensuing', 'mute', 'downey', 'cathy', 'montana', 'happenstance', 'goldberg', 'confidant', 'nails', 'doctors', '1982', 'farting', 'eaten', 'vince', 'rhymer', 'rotting', 'olds', 'scientologists', 'repellent', 'nicest', 'hernandez', 'luis', 'sweetest', 'oddest', 'innate', 'novak', 'sullavan', 'auspicious', 'meetings', 'plotline', 'expertly', 'fatherly', 'hugo', 'pirovitch', 'bressart', 'errand', 'unpretentious', 'lovebirds', 'fledged', 'boyish', 'equate', 'sincerity', 'fares', 'meek', 'loyal', 'wallet', 'lavish', 'meticulous', \"you've\", 'mail', 'detract', 'harm', 'impeccable', 'layers', 'peel', 'reflection', 'staggering', 'elude', 'characteristics', 'declaration', 'devotion', 'peck', 'fearless', 'tempered', 'ordered', 'babylon', '1940', 'madre', 'ranges', 'dwight', 'yoakam', 'ordinarily', 'intervenes', 'grips', 'nightmarish', 'disguised', 'postscript', 'golf', 'confined', '24', 'distinguishes', 'emoting', 'formulaic', 'inherit', 'heiress', 'thrilled', 'thursday', 'cadillac', 'stepmother', 'waters', 'isle', 'cukor', 'distinctive', 'ronald', 'deed', 'faultless', 'collins', 'sawyer', 'bissell', 'fascination', 'quinn', 'hogg', 'biography', 'stardom', 'giants', 'messy', 'abundance', 'minimally', 'admission', 'album', 'warhols', 'exploration', 'volatile', 'exceeded', 'prejudice', 'midget', 'caveman', 'resorted', 'insurance', 'laughlin', 'vivah', 'kapoor', 'tivo', 'yorkers', 'ronde', 'peeing', 'aroused', 'roof', 'confesses', 'praises', 'promises', 'groin', 'smitten', '1000', 'lunch', 'bench', 'shtick', 'tomorrow', 'arriving', 'rolls', 'untrue', 'redneck', 'blast', '93', 'jaw', 'racially', 'diverse', 'balanced', 'sadism', 'tormentors', 'phrase', 'connecting', 'cd', 'puzzle', 'spiers', 'hammy', 'exhilarating', 'classroom', 'arthouse', 'invite', 'absorb', 'savor', 'strike', 'bins', 'wondrous', 'shameful', 'suspicions', 'duke', 'denver', 'cecil', 'decadent', 'regeneration', 'district', 'attorney', 'skill', 'piles', 'furs', 'gesturing', 'twitching', 'enlivened', 'lois', 'naturally', 'tearful', 'moralizing', 'card', 'moralistic', 'drones', 'stance', 'rome', 'unknowing', 'archaic', 'placement', 'handling', 'voted', 'penal', '1942', 'divorce', 'eventual', 'separation', 'estranged', 'divorced', 'husbands', 'colorless', 'sanders', 'pose', 'furniture', 'sexiness', 'fires', 'irving', 'miniver', 'mega', 'mrs', 'burnt', 'altogether', '40s', 'dud', 'tattoo', 'mannerisms', 'diaz', 'offs', 'hijack', 'improvement', 'airplane', 'flight', 'vigalondo', 'min', 'fay', 'continuation', 'outright', 'pleasing', 'decently', 'psychotic', 'shootings', 'respecting', 'marketed', 'penis', 'floors', 'january', 'bearer', 'extend', 'abiding', 'attain', 'credentials', 'willingness', 'passionately', 'mayhem', 'nomadic', 'sights', 'surrender', 'suspecting', 'charmer', 'virgins', 'feeding', 'mama', 'krige', 'mädchen', 'displaying', 'bitch', 'wipes', 'corn', 'unconscious', 'tosses', 'stabbed', 'fence', 'cats', 'hooper', 'officers', 'photograph', 'complexity', 'overdrive', 'prejudices', 'grievances', 'superiority', 'overweight', 'languages', 'funded', 'eyebrow', 'forehead', 'meal', 'appallingly', 'exaggerations', 'behaviour', 'customs', 'thinly', 'veiled', 'incest', 'heavier', 'subway', 'crazed', 'terrorists', 'uber', 'ascribe', 'islam', 'interviewing', 'christianity', 'sirhan', 'scenario', 'existential', 'yourselves', 'cope', 'cure', 'awakens', '200', 'contemporaries', 'raiders', 'ark', 'bruckheimer', 'idealized', 'elusive', 'eleanor', 'angelina', 'giovanni', 'ribisi', 'wary', 'drivers', 'cinematographic', 'a+', 'cappy', 'checks', 'olivia', 'd`abo', 'unseen', 'wander', 'keen', 'goods', 'horse', 'confess', 'alec', 'discussions', 'wound', 'supports', 'apologizing', 'proclaims', 'inexcusable', 'fabrication', 'portrait', 'civilian', 'staples', 'ova', 'scattered', 'surpasses', 'craving', 'mandatory', 'preference', 'caucasian', 'instilling', 'impress', 'battlefield', 'funky', 'avenue', 'lynch', 'blended', 'dragging', 'stroke', 'unpredictable', 'admitted', 'overwhelming', 'virginity', 'cloak', 'psychedelic', 'kamikaze', 'laced', 'viggo', 'convict', 'ostensibly', 'inflection', 'dimensional', 'photogenic', 'prominently', 'veterans', 'turgid', 'wobbly', 'comfort', 'council', 'grants', 'measure', 'badness', 'baggy', 'furry', 'carpeting', 'deny', 'baths', 'raunchy', 'bergman', 'drugged', 'fortunate', 'jumped', 'legacy', 'abomination', 'archenemy', '_', 'sheet', 'ely', 'leaping', 'brick', 'blasting', 'pistol', 'smack', 'paramount', 'woke', 'printed', 'jocks', 'pranks', 'jester', 'represent', 'foes', 'blockbuster', 'glamour', 'f.', 'philipps', 'psyche', 'dramatics', 'aggression', 'comparing', 'snarling', 'charlton', 'mantegna', 'circumstance', 'significance', 'marks', 'consciousness', 'september', 'firefighter', 'arrived', 'collapsed', 'cameras', 'turmoil', 'phase', 'rubble', 'recreated', 'witnessing', 'safety', 'arguing', 'frazee', 'riders', 'sage', '~', 'strauss', 'reed', 'goodman', 'gwynne', 'sid', 'caesar', 'yvonne', 'carlo', 'herman', 'grandpa', 'terrorize', 'continuing', 'freeze', 'widely', 'tent', 'dust', 'requests', 'meiks', \"o'leary\", 'brazilian', 'mão', 'diabo', 'distributor', 'eight', 'brazil', 'harilal', 'chubby', 'aids', 'fears', 'reminding', 'strict', 'kidding', 'nonstop', 'asimov', 'rescued', 'afterthought', 'pondering', 'kinkade', \"o'clock\", 'banal', 'northwest', 'fishing', 'congested', 'insights', 'achieves', 'goals', 'posses', 'waterbury', 'collectors', 'mainstream', '2003', 'stream', 'screens', 'novelty', 'sherriff', 'intrigued', 'nondescript', 'inadequate', 'muttering', 'narcissistic', 'barring', 'clarence', 'bird', 'loren', 'columbine', 'diary', 'pipe', 'mclaglen', 'skywalker', 'r2-d2', 'raj', 'kissing', 'matinee', 'greeted', 'unwatchable', 'contest', 'exploding', 'reeks', 'adored', 'avengers', 'idol', 'tracking', 'kruger', 'horrifying', '2000', 'lovingly', 'ricky', 'toothless', 'fathers', 'teams', 'epps', 'pert', 'blink', 'netflix', 'litter', 'housekeeper', 'misplaced', 'props', 'contributing', 'exchange', 'antonioni', 'distraction', 'alienation', 'materialism', 'goldsworthy', 'expressing', 'fundamentally', 'gentle', 'inability', 'outdoors', 'gift', 'visions', 'utmost', 'wandering', 'disgraceful', 'sequal', 'w', 'sarcasm', 'saddest', 'reservations', 'ideally', 'imaginable', 'rivals', 'excellently', 'denouement', 'i.q.', 'dreamgirls', 'glorified', 'portion', 'momentum', 'formless', 'condon', 'caliber', 'grease', 'graduation', 'darlene', 'busted', 'smuggling', '33', 'endings', 'owning', 'beckinsales', ';)', 'brokedown', 'bezukhov', 'lean', 'scarecrow', 'insomnia', 'bolkonsky', 'unimpressive', 'turd', 'exaggerated', 'uncomfortably', 'flowing', 'dramatically', 'salary', 'deteriorated', 'extent', 'ours', 'homophobic', 'pretending', 'kissed', 'perfection', 'brushes', 'loosely', 'woodenhead', 'avenge', 'waffle', 'touchy', 'overbearing', 'suitably', 'factors', 'headache', 'grisly', 'commendably', 'lock', 'smoking', 'heading', 'lurking', 'afoul', 'spiteful', 'nobodies', 'alarming', 'chords', 'onwards', 'endlessly', 'courtesy', 'shia', 'encourage', 'j.p.', 'heavenly', 'knox', 'danny', 'glover', 'pitcher', 'strays', 'landon', 'drilling', 'threatens', 'utah', 'stare', 'catered', 'mormons', 'borderline', 'unbearable', 'sewn', 'tolerance', 'wounds', 'clyde', 'flashy', 'stereotyped', 'dishonest', 'chooses', 'whining', 'kings', 'victimized', 'rescues', 'banality', 'lucio', 'fulci', 'charisma', 'illiteracy', 'tool', 'toned', 'swallow', 'thompson', 'dodgy', 'dern', 'calendar', 'tripod', 'sufficient', 'lensman', 'kiddie', 'harvester', 'unharvested', 'dumping', 'silage', 'crops', 'grain', 'lifted', 'plunges', 'shrieks', 'shout', 'd.j.', 'graffiti', 'exploded', 'emphasizes', 'naïve', 'outline', 'showtime', 'rendered', 'tripe', 'fixing', 'hepburn', 'gilbert', 'artifacts', 'vault', 'walmart', 'ashes', 'amid', 'palm', 'vermont', 'estate', 'perry', 'drove', 'quincy', 'capital', 'nominees', 'raises', 'standpoint', 'homo', 'eroticism', 'interaction', 'audiard', 'ignored', 'progressively', 'gesture', 'spooky', 'marsden', 'speedman', 'challenging', 'apathetic', 'momentarily', 'smacks', 'pretension', 'soapy', 'concern', 'muscle', 'plausible', 'wahlberg', 'chuck', 'metaphorically', 'fiasco', 'likelihood', 'simultaneously', 'knuckle', 'preparation', 'jagger', 'hairdo', 'announces', 'engagement', '3000', 'zombification', 'agar', 'embraces', 'artistry', 'formed', 'borrows', 'madge', 'madeline', '1943', 'rainbow', '1988', '28', '2002', 'bottle', 'translate', 'dilemma', 'curse', 'nations', 'nu', 'ler', 'maintaining', 'delirious', 'intrinsic', 'inhabits', 'alternative', 'voigt', 'foremost', 'gritty', 'cowboy', 'm*a*s*h', 'jordan', 'bros.', 'trivia', 'introduce', 'crouse', 'illinois', 'townsfolk', 'aplenty', 'uncaring', 'drone', 'applauded', 'scripting', 'c.', 'hickok', 'a.', 'virginia', 'errol', 'request', 'presidency', 'pope', 'monroe', 'yamamoto', 'commenting', 'homicidal', 'transform', 'amemiya', 'necroborgs', 'stunningly', 'usage', 'transforms', 'iron', 'satirical', 'shamefully', 'migenes', 'adaptations', 'gestures', 'psychobabble', 'parked', 'tree', 'zoom', 'chrysler', 'dozed', 'alain', 'swift', 'laconic', 'refused', 'retire', 'bury', 'hatchet', 'automobile', 'yard', 'crushed', 'compositions', 'competently', 'ax', 'claiming', 'selected', 'unsettling', 'sumpter', 'pan', 'issued', 'sentence', 'paychecks', 'sandu', 'tudor', 'piercing', 'stardust', 'eleven', 'wichita', 'anal', 'confession', 'confessed', 'methods', 'indulged', 'recreation', 'corpses', 'regiment', 'uniform', 'recruiting', 'sergeant', 'charges', 'pompadour', 'blouses', 'irreverent', 'rogue', 'swordfights', 'roofs', 'joyous', 'optimistic', 'iconic', 'francois', 'pages', 'graceful', 'dreamed', 'sworn', 'luc', 'detest', 'targets', 'pointlessness', 'angst', 'cineastes', 'brag', 'maverick', 'claudia', 'helga', 'sinclair', 'detectives', 'hid', 'formidable', 'hatred', 'huston', 'tamblyn', 'farrel', 'slackers', 'respectively', 'exam', 'unlikeable', 'schwartzman', 'ram', 'stunned', 'leadership', 'begs', 'namely', 'travels', 'rituals', 'verdict', 'elm', 'norm', 'playboy', 'jilted', 'uniformly', 'raw', 'disdain', 'atlanta', 'assure', 'rendering', 'laurence', 'overrated', 'drosselmeyer', 'unsophisticated', 'relish', 'wig', 'amateurishly', 'haley', 'sugar', 'plum', 'additional', 'tchaikovsky', 'loathe', 'terry', 'willis', 'prevent', '1995', 'harmless', 'stowe', 'cheering', 'zelda', 'trinity', 'youngest', 'betrayed', 'deciding', 'koteas', 'roots', 'realization', 'prolonged', 'neil', 'ross', 'cheaper', 'entertainers', 'instant', 'lifeless', 'knocked', 'milk', 'pointlessly', 'fruitless', 'denmark', 'carnosaurs', 'dinosaurs', 'commandos', 'invented', '2005', 'positively', 'seminal', 'literature', 'catching', 'immunity', 'widescreen', 'lengths', 'sepia', 'variations', 'martian', 'resist', 'dwell', 'affliction', 'senseless', 'insufferable', 'lasts', 'builds', 'hunky', 'kaufman', 'w.', 'bernard', 'competitors', 'hallucinations', 'fascinated', 'interviewed', 'accused', 'croatia', 'serbia', 'conflicts', 'mentioning', 'seattle', 'civilians', 'yugoslav', 'holocaust', 'bombed', 'muslim', 'macedonians', 'radicals', 'understands', 'neutral', 'vile', 'befriends', 'spread', 'rivers', 'aesthetic', 'legally', 'disorganized', 'primed', 'molested', 'dudley', 'compromise', 'awesomely', 'nagging', 'pier', 'impeccably', 'horseback', 'chopped', 'imprisoned', 'similarity', 'spouts', 'veers', 'traumatised', 'shoulders', 'wrongs', 'hardened', 'irs', 'frustrations', 'unravel', 'cheapest', '-vessel', 'poop', 'envy', 'questioning', 'artistically', 'roberts', 'scorpion', 'tail', 'martino', 'argento', \"'cause\", 'murderers', 'hilton', 'modine', 'sematary', 'toddler', 'gage', 'cemetery', 'differs', 'sickening', 'hounds', 'shocker', 'difficulties', 'moss', 'hart', 'shelly', 'pit', 'err', 'corners', 'connie', 'advertising', 'executive', 'financial', 'gung', 'heed', 'buys', 'dealer', 'sucking', 'zis', 'architect', 'reginald', 'escalating', 'appointed', 'wham', 'spam', 'pink', 'jacket', 'attentions', 'blameless', 'forgetting', 'mishaps', 'shade', 'stealth', 'rounded', 'leaders', 'creators', 'helping', 'gunshots', 'stab', 'alluded', 'artillery', 'predator', 'amounts', 'fiends', 'shut', 'hallways', 'warmth', 'spawned', 'vertical', 'jumping', 'paulsen', 'luxurious', 'rushton', 'crashes', 'passage', 'starship', 'sara', 'implications', 'marlon', 'subconscious', 'decorated', 'epilepsy', 'decoration', 'sanity', 'putrid', 'referring', 'unwisely', 'parking', 'redundant', 'bitching', 'incoherent', 'kit', 'indicate', 'fangoria', 'cake', 'getaway', 'delights', 'expensive', 'participate', 'wirtanen', 'regime', 'acknowledge', 'haunts', 'patriot', 'surrounds', 'se', 'wholeheartedly', 'horrid', \"i'm\", '23', 'unit', 'clunker', 'buchfellner', 'worship', 'lurks', 'murky', 'zooms', 'fisted', 'hurry', '1934', 'frivolous', 'cockney', 'unclear', 'shuddering', 'repetition', 'strongest', 'curiously', 'despises', 'masochistic', 'enslaved', 'champagne', 'hunchback', 'picnic', 'gorilla', 'visceral', 'kilmer', 'dylan', 'boogie', 'wonderland', 'nastiest', 'unharmed', 'steaming', 'garland', 'striesand', 'peaked', 'bogdanovich', 'gag', 'fur', 'chilly', 'styles', 'sweater', 'soil', 'b&w', 'goofs', 'performs', 'byplay', 'awaiting', 'polar', 'refrigerator', 'd.', 'spine', 'scatman', 'crothers', 'palance', 'exterior', 'physique', 'pattern', 'holster', 'shakespearian', 'regal', 'unwise', 'unnerving', 'reels', 'contradict', 'thorsen', 'manipulation', 'coups', 'marine', 'opposing', 'filter', 'economy', 'pockets', 'noses', 'traced', 'moe', 'thelma', 'ritter', 'coburn', 'lawman', 'describing', 'akin', 'vinny', 'liquid', 'afro', 'gossipy', 'family-', 'facility', 'housing', 'lumbering', 'humble', 'dv', 'newer', 'zulu', 'planted', '400', '10,000', 'conclude', 'rickety', 'theories', 'zadora', 'ineptly', 'soo', 'windshield', 'hbo', '1991', 'ruts', 'chi', 'haunt', 'unscathed', 'shone', 'moderate', 'eighties', 'agencies', 'attackers', 'vanished', 'blackmailed', 'tricked', 'glorious', 'defeated', 'favour', 'carpenter', 'dusk', 'dawn\"-audience', 'jover', 'avoiding', 'murnau', 'expressive', 'fills', 'oft', 'thespian', 'besotted', 'investigating', 'garson', 'blessed', 'gaining', 'mitchell', 'abuses', 'brooke', 'intricacies', 'bonaparte', 'mixture', 'marked', 'rebane', 'eccentric', 'incomprehensible', 'pornographic', 'dickinson', 'caine', 'admits', 'unfulfilled', 'suggested', 'intrusive', 'shower', 'dogtown', 'surfed', 'halla', 'bol', 'rang', 'basanti', 'mentor', 'santoshi', 'indulges', 'abortion', 'infidelity', 'juggling', 'imo', 'listener', 'beside', 'montgomery', 'hurts', 'es', 'einmal', 'das', 'reproduction', 'tasteful', 'coverage', 'oxygen', 'investment', 'pedestrian', 'plods', 'cadence', 'ceilings', 'impractical', 'marvellous', 'animators', 'explore', 'stylistic', 'sumptuous', 'graphically', 'farnham', 'cohorts', 'comeback', 'starry', 'belt', 'swapping', 'phoned', 'mcshane', 'farell', 'selma', 'hayek', '-and', 'worn', 'affleck', 'walthall', 'egan', 'kindness', 'd.w.', '13', 'graduates', 'finer', 'dancers', 'rohmer', 'aristocratic', 'emerges', 'staunch', 'assembly', 'equality', 'maid', 'justification', 'keenly', 'madness', 'existing', 'searched', 'servants', 'incapable', 'amply', 'indonesian', 'necessity', 'bowls', 'surge', 'shops', 'debuted', 'designer', 'tiny', 'challenged', 'premises', 'initiation', 'busby', 'scar', 'spaghetti', 'flavor', 'sanchez', 'cadavers', 'collect', 'exchanging', 'conniving', 'saloon', 'pepe', 'reasonably', 'henchman', 'anticipation', 'gunfight', 'retreat', 'consist', 'parties', 'cameraman', 'ti', 'sfx', 'roach', 'offered', 'rko', 'royale', 'davies', 'sydney', 'excuses', 'diabolical', 'biopic', 'chunks', 'unaccountably', 'imposing', 'ignoring', 'speakers', 'incongruous', 'basing', 'guevara', 'allotted', 'assignment', 'revolutionaries', 'telegraphed', 'castro', 'officials', 'cinemas', 'sorely', 'campaign', 'bolivia', 'travelogue', 'painting', 'interminable', 'shockingly', 'terminator', 'implausibility', 'strengthens', 'charley', 'wraps', 'lwr', 'racing', 'motorcycle', 'largest', 'burgi', 'cos', 'searing', 'mourning', 'schrader', 'doris', 'rosenstrasse', 'alert', 'gestapo', 'jewish', 'lohde', 'riemann', 'inhumanity', 'retelling', 'detracted', 'lighter', 'teaming', 'wodehouse', 'preview', 'missions', 'poverty', 'damita', 'brokeback', 'misogyny', 'arrow', 'cradled', 'tables', 'captors', 'businessman', 'metallica', 'wayside', 'arguments', 'easier', 'tails', 'thirties', 'masted', 'bates', 'i.', 'venturing', 'cunning', 'whimsy', 'longed', 'affluent', 'boyhood', 'womanizing', 'torch', 'lurch', 'trend', 'articles', 'changeling', 'inviting', 'invited', 'lab', 'remained', 'fancy', 'spill', 'beans', 'assumptions', 'tribal', 'uh', 'ambiguous', 'sums', 'argues', 'gargantuan', 'declares', 'polished', 'canceled', 'combines', 'outtakes', 'leg', 'descent', 'chad', 'fractured', 'sexuality', 'lorne', 'greene', 'replace', '1965', 'preachy', 'counter', 'hawaii', 'chances', 'officially', 'choir', 'jewel', 'd&d', 'airships', 'blimps', 'considerably', '5,000', 'fond', 'meanings', 'happier', 'default', 'fellini', 'bachelor', 'zealand', 'glances', 'condescending', 'aura', 'advised', 'ing', 'tones', 'vapid', 'indistinguishable', 'upstage', 'intrigue', 'gifts', 'caressing', 'winded', 'addressed', 'commend', 'staggeringly', 'magazines', 'bow', 'arguably', '17th', 'shogun', 'kosugi', 'tenchu', 'strife', 'pans', 'arises', 'birds', 'pretentiousness', 'defendants', 'intelligently', 'accented', 'memorized', 'distributors', 'intro', 'edits', 'amateur', 'accuracy', 'macho', 'cloney', 'pledge', 'stafford', 'heritage', 'agnostic', 'karl', 'flock', 'sheep', 'shattered', 'bailey', 'thee', 'chalice', 'embracing', 'shimmering', 'mecca', 'growth', 'confidence', 'assured', 'lifelong', 'impatient', 'toss', 'selection', 'appreciation', 'lunatic', 'yankees', 'drown', 'preacher', 'jezebel', 'disrobing', 'contests', 'wrought', 'greatness', 'error', 'roses', 'banks', 'siege', 'chastity', 'stacey', 'helmets', 'trivial', 'marina', 'hrothgar', 'facilitate', 'distract', 'encouraged', 'retain', 'shred', 'physics', 'artisans', 'unferth', 'astounding', 'timed', 'inherited', 'recessive', 'scratches', 'bleed', 'clinic', 'smell', 'coffee', 'scotch', 'sober', 'assist', 'sends', 'orbit', 'saturn', 'colonel', 'monty', 'python', 'peaks', 'consideration', 'wickedly', 'degrees', 'addicted', 'whitney', 'vaulted', 'atlantic', 'users', 'cologne', 'followers', 'promotional', 'squarely', 'larry', 'melancholy', 'techno', 'latin', 'brainer', 'bolan', 'whimsical', 'starr', 'dani', 'august', 'overlooked', 'conveyed', 'superheroes', 'underwhelming', 'diana', 'amazon', 'crossed', 'expanded', 'caution', 'mian', 'mai', 'ziyi', 'tung', 'vitality', 'inn', 'wirework', 'arrows', 'climbing', 'await', 'paltrow', 'intentioned', 'northam', 'poise', 'nobility', 'revolver', 'monologue', 'colonial', 'bosses', 'oblivious', 'impressionable', 'december', 'richly', 'wholesome', 'holidays', 'pumped', 'insensitive', 'disconnected', 'cloud', 'insanely', 'mortal', 'subtitles', 'rhonda', 'dubious', 'despicable', 'madison', 'maysles', 'rivera', 'gardens', 'yorker', 'richest', 'bouvier', 'lasting', 'opinions', 'protector', 'alosio', 'biker', 'signs', 'portrayals', 'rooftop', 'dalmar', 'ivanna', 'experiments', 'cells', 'char', 'broiled', 'arch', 'lybbert', 'hopalong', 'grungy', 'volckman', 'karas', 'monochrome', 'imitate', 'sources', 'slightest', 'ashore', 'penned', 'secondary', 'parisian', 'records', 'jake', 'gyllenhaal', 'satellite', 'shaft', 'signal', 'elvis', 'sherman', 'foreman', 'natalie', 'creek', 'scholarship', 'determination', 'contrasts', 'symbolize', 'grays', 'guided', 'towns', 'harassment', 'shantytown', 'publish', 'manuscript', 'zimbabwe', 'banned', 'unleashes', 'jenifer', 'drain', 'jock', 'stripper', 'duties', '1935', 'knees', 'partners', 'hostess', 'ballroom', 'harriet', 'hilliard', 'bilge', 'screwball', 'monkey', 'audition', 'drinks', 'clashing', 'bela', 'janos', 'poisoned', 'curing', 'frances', 'lawton', 'fury', 'claudette', 'vance', ':-)', 'vocal', 'monotone', '1962', 'bomber', 'garcia', 'chandler', 'clooney', 'tube', 'nap', 'spare', 'limbs', 'shemp', 'deliciously', 'goof', 'alley', 'apologies', 'infused', 'contemplate', 'fated', 'ethereal', 'gentlemen', 'bothering', 'tiger', 'toby', 'restless', 'olsen', 'kirsten', 'dunst', 'sophie', 'alienated', 'rom', 'com', 'brigade', 'coffin', 'revival', 'deservedly', 'obscurity', 'kensett', 'overstatement', 'uli', 'nauseous', 'stole', 'carrey', 'morale', 'enthusiastic', 'horner', 'willy', \"o'donnell\", 'imagining', 'commendable', 'judgment', 'resonance', 'paternal', 'culled', 'motif', 'hangs', 'conrad', 'splattered', 'malevolent', 'jude', 'insomniac', 'awake', 'penny', 'warnings', 'millennium', 'joked', 'prepare', 'mans', 'scotland', 'northern', 'stilted', 'flies', 'net', 'spout', 'hg', 'someplace', 'elizabeth', 'thumb', 'dedication', 'interpretive', 'sopranos', 'safely', 'comatose', 'salesman', 'option', 'confirmed', 'a.j.', 'carmella', 'approaches', 'marries', 'roller', 'fungicide', 'unleashing', 'film\"-makers', 'imbeciles', 'manically', 'attaching', 'fuse', 'explodes', 'envelope', 'bicycle', 'pump', '1984', 'laziness', 'stupidly', 'inaccurate', 'ate', 'digest', 'macy', 'jeter', 'rites', 'crossbreed', 'freezer', 'outer', 'admittedly', 'plesiosaur', 'wreak', 'havoc', 'superheating', 'resting', 'dino', 'strangers', 'liquor', 'booze', 'forms', 'hicks', 'intentional', 'renard', 'bertille', 'noel', 'establish', 'seasons', 'amusement', 'notions', 'undergone', 'wrap', 'clip', 'scope', 'farcical', 'hastily', 'hateful', 'spaces', 'undone', 'gushing', 'controller', 'spared', 'refer', 'esquire', 'y', 'staircase', 'enigma', 'artists', 'exhibition', 'sandler', 'dumped', 'sunset', 'rigid', '90210', '16', 'realise', 'lined', 'wrinkles', 'shorts', 'kissy', 'bursting', 'coloured', 'torturing', 'setups', 'unscrupulous', 'khan', 'influenced', 'brazenly', 'swayze', 'netherlands', 'curl', 'sack', 'beaton', 'stalker', 'bagman', 'prom', 'finely', 'courageous', 'surviving', 'spilled', 'pour', 'fetish', 'chaplin', 'reflects', 'socio', 'tangible', 'callahan', 'couples', 'lerner', 'clumsily', 'robber', 'carole', 'lombard', 'talky', 'oriented', 'minus', 'recite', 'marvelous', 'spacey', 'prototype', 'arid', 'midwestern', 'updated', 'spree', 'admire', 'vic', 'warden', 'illegal', 'shady', 'sweeps', 'sunny', 'inevitability', 'repelled', 'humbly', 'grabovsky', 'reconciliation', 'alienating', 'miraculous', 'rival', 'undying', 'juxtaposed', 'serve', 'wimp', 'ingratiating', 'chop', 'etched', 'mears', 'mile', 'remembers', 'shootouts', 'purist', \"c'mon\", 'outdoor', 'moth', 'slows', 'generations', 'copying', 'tearing', 'concrete', 'starved', 'terror', 'adequately', 'slapped', 'hooker', 'hallucination', 'deleted', 'askey', 'octane', 'motley', 'railway', 'bach', 'offbeat', 'solitude', 'realms', 'artsy', 'frantic', 'sentinel', 'cristina', 'portal', 'vatican', 'ava', 'gardner', 'ferrer', 'berenger', 'orbach', 'possessed', 'imaginative', 'eyebrows', 'seduction', 'haddock', '68', 'shocks', 'cuban', 'communism', 'trite', 'tasteless', 'trunk', 'hammer', 'lousy', 'defence', 'restaurant', 'shines', 'iris', 'investigation', 'commission', 'investigated', 'sung', 'andrews', 'olin', 'centre', 'elaborate', 'disorder', 'il', 'untergang', 'mamie', 'selfless', 'foregoing', 'natassia', 'warehouse', 'port', 'mathau', 'grumpy', 'messing', 'disappearance', 'affectionately', 'ineptness', 'ju', 'tokyo', 'chim', 'predictably', 'karen', 'slayer', 'godzilla', 'enhancements', 'competitive', 'scheduled', 'inc.', 'devotee', 'philosophical', 'firmly', 'mixing', 'imitating', 'gonzalo', 'unexpectedly', 'berth', 'capturing', 'wider', 'instincts', 'lest', 'judged', 'wright', 'tremendous', 'eli', 'wallach', 'unsatisfying', 'intestines', 'faked', 'replacing', 'map', 'response', 'irrefutable', 'economically', 'penance', 'dates', 'hacked', 'originals', 'skipping', 'georges', 'roquevert', 'philippe', '1951', 'sample', 'marcel', 'programming', 'tentative', 'screenings', 'bardot', 'grandparents', 'disappearing', 'peculiar', 'marthe', 'reverand', 'martha', 'dara', 'tomanovich', 'kirkland', 'emphasize', 'bel', 'tremor', 'overacting', \"i'll\", 'applaud', 'behaviors', 'todays', 'bafta', 'damaged', 'limp', 'lackluster', 'eminently', 'dynamics', 'thunderbirds', 'lian', 'chu', 'gwizdo', 'hires', 'contender', '2008', 'expelled', 'prejudiced', 'politically', 'deem', 'orwellian', 'secular', 'unwillingness', 'efficiently', 'impressively', 'framework', 'ludicrously', 'balsam', 'squeeze', 'goings', 'gil', 'orchestral', 'wisecracks', 'banter', 'fable', 'exposes', 'acquainted', 'motivation', 'bets', 'traditionally', 'lazy', 'sluggish', 'ships', 'ninjas', 'heflin', 'nielsen', 'temporary', 'wheeler', 'doorman', 'snappy', 'kibbutzim', 'entrance', 'socialist', 'zen', 'coasted', 'boil', 'bee', 'shintaro', 'palette', 'violet', 'restores', 'drastically', 'absurdism', 'transcend', 'staging', 'initially', 'karate', 'fish', 'waterfalls', 'villages', 'ryuhei', 'kitamura', 'interplay', 'characterisation', 'aya', 'chiaki', 'kuriyama', 'notion', 'hyperbole', '250', 'insider', 'pov', 'recounting', 'loudly', 'diners', 'candies', 'choreographer', 'circuit', 'perrine', 'finance', 'decency', 'rewrites', 'neutered', 'hanzô', 'constable', 'assistants', 'occurrence', 'trains', 'camping', 'yells', 'doorstep', 'severed', 'automobiles', 'crooked', 'marrying', 'milland', 'rung', 'motive', 'advance', 'favors', 'owed', 'distracted', 'bitten', 'chico', 'nietszche', 'declared', 'donor', 'recipient', 'militaries', 'discipline', 'unfathomable', 'monteith', 'thereby', 'tag', 'immersed', 'leonardo', 'dicaprio', 'emanating', 'restrain', 'stakes', 'slice', 'naturalism', 'lads', 'marvelously', 'smoothly', 'unfolding', 'marleen', 'attic', 'turkey', 'pumping', 'stallone', 'quotient', 'sweat', 'covert', 'mias', 'vietnamese', 'extract', 'introspective', 'relies', 'wraiths', 'cheery', 'derived', 'irresistible', 'aggressive', 'rené', 'minority', 'scape', 'holroyd', 'queenie', 'lemmon', 'shep', 'bride', 'lust', 'blush', 'westfront', '1918', 'settings', 'wwi', 'blabber', 'elmo', 'reject', 's.s.', 'taping', 'recession', 'watanabe', 'personifies', 'brash', 'overshadows', 'affable', 'cornball', 'surfers', 'sundance', 'waves', 'adrenaline', 'choosing', 'heroines', 'schmid', 'dirt', 'overseas', 'banning', 'tomb', 'andoheb', 'caretaker', 'fiancé', 'kidnap', 'isobel', 'bannings', 'antagonist', 'robotboy', 'southeast', 'congressman', 'damned', 'ned', 'quaid', 'dourif', '911', 'operator', 'yea', 'entitled', 'bu', 'exchanges', 'efficient', 'blackie', 'cassandra', 'peterson', 'fixation', 'forbidden', 'critic', 'seventeen', 'rasuk', 'rodriguez', 'arise', 'grandma', 'latina', 'complimented', 'indoor', 'ethnicity', 'teeny', 'sharing', 'bittersweet', 'sabotaged', 'grits', 'prophetic', 'heyday', 'exaggerating', 'hilary', 'hauser(cusack', 'tomeii', 'operating', 'patter', 'gromit', 'rita', 'twins', 'tattooed', 'corri', 'blinds', 'grabs', 'decor', 'sybil', 'danning', 'opus', 'cream', 'gals', 'brainwashed', 'belinda', 'mayne', 'advise', 'antoine', 'depardieu', 'deneuve', 'morocco', 'backward', \"l'art\", 'agreed', '1947', 'weinberg', 'tenth', 'materials', 'simpler', 's-50', 'k-25', 'established', 'november', 'carbide', 'prussia', '1902', 'principles', 'applications', 'führer', '1983', 'unquestionably', 'boxing', 'opponent', 'truthfully', 'attitudes', 'sketch', 'moreover', 'pauline', 'chinnery', 'lays', 'colours', 'matches', 'slang', 'definition', 'flames', 'ishtar', 'niel', 'dresses', 'surrounded', 'nun', 'lynn', 'absurdly', 'babies', 'cooking', 'spun', 'tenant', 'subjects', 'muir', 'drills', 'mcgavin', 'electrical', 'r.i.p.', 'gilles', 'lancré', 'faint', 'rickshaw', 'domergue', 'demand', 'winds', 'depicting', 'autobiography', 'texan', 'toying', 'surfaces', 'opportunities', 'codger', 'debts', 'paste', 'bell', 'mechanical', 'funeral', 'pretend', 'quit', 'pirate', 'comrades', 'unintentional', 'sachs', 'respectable', 't.v', 'cab', 'harvest', 'kimmel', 'urmila', 'hats', 'burying', 'hulk', 'identify', 'antz', 'bosox', 'scarlett', 'advertised', 'figured', 'dutta', 'hitchcock', 'pg-13', 'hancock', 'fearing', 'oozes', 'suprematy', 'patriots', 'tanger', 'agenda', 'thrill', 'antwone', 'antonella', 'ríos', 'commented', 'millimeter', 'cerebral', 'leisen', 'syndicate', 'unknowingly', 'uni', 'reliving', 'bigfoot', 'alliance', 'slave', 'sarlacc', 'obi', 'wan', 'exploitive', 'concepts', 'idiocy', 'ants', 'knocks', 'astounded', 'sun', 'kerry', 'joycelyn', 'sophomore', 'steppers', 'terrell', 'blend', 'voters', 'unintelligible', 'wrestler', 'hayao', 'resolve', 'mmpr', 'cliched', 'ada', 'nonsensical', 'luring', 'proceeds', 'spitting', 'accepting', 'dale', 'castaways', 'backyard', 'carson', 'scalpel', 'kidd', 'flo', 'healy', 'brat', 'mpaa', 'gooding', 'gunfire', 'scored', 'stimulating', 'viola', 'bishop', 'stasi', 'resident', 'prosthetics', 'cons', 'rican', 'ricans', 'guarding', 'gateway', 'archdiocese', 'astor', 'lancaster', 'jenna', 'crashing', 'rms', 'niki', 'ardelean', 'tenny', 'phyllis', 'berkoff', 'hoechlin', 'sorta', 'files', 'mcdowall', 'prostate', 'smudge', 'spurlock', \"d'abo\", 'amitabh', 'xxfake']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6S-QRmsNx_c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f6e2d296-8cbf-4a24-a822-f5736f8456dd"
      },
      "source": [
        "data_clas = TextClasDataBunch.from_csv(path, 'texts.csv', vocab=data_lm.train_ds.vocab, bs=32)\n",
        "print(type(data_clas))\n",
        "# print(data_clas)\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'fastai.text.data.TextClasDataBunch'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeS8g9frE574"
      },
      "source": [
        "data_clas.save('data_clas_export.pkl')"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABq5_Mk4E7sO",
        "outputId": "7c4d94c4-d414-4915-9728-a4aed7bee33d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_clas = load_data(path, 'data_clas_export.pkl', bs=bs)\n"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvpo3BUzE9uW",
        "outputId": "454d1c96-26e8-41d8-bafc-e74aad136bc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5).to_fp16()\n",
        "learn_c.load_encoder('ft_enc')"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNLearner(data=TextClasDataBunch;\n",
              "\n",
              "Train: LabelList (799 items)\n",
              "x: TextList\n",
              "xxbos xxmaj evil xxmaj breed is a very strange slasher flick that is unfortunately no good . xxmaj the beginning of the film seems promising but overall it 's a disaster . xxmaj the dialogue is pretty bad but not near as bad as the acting . xxmaj the acting is brutal and unbearable . xxmaj most of the characters deliver there lines horribly and even if that is on purpose the method does n't work because the characters become annoying . xxmaj some of the kills are innovative but it took far too long to get to them . xxmaj after about a half hour through the movie we get the first death ( other than in the xxunk then almost every other character is smoked within the next five minutes . xxmaj the movie then turned into sort of a spoof with ridiculous looking characters , unrealistic karate like fights , and a scene in which a man gets his intestines pulled out of his xxunk . xxmaj none of it is funny it 's just plain ridiculous . xxmaj the film then becomes ultra gory and ultra pointless . xxmaj most of the characters are clichéd even for slasher standards and are as solid as xxunk left on the counter for 5 days . xxmaj evil xxmaj breed is n't even laughably bad therefore it fails in it 's main task . xxmaj watch xxmaj texas xxmaj chainsaw xxmaj massacre , xxmaj just xxmaj before xxmaj dawn , or xxmaj see xxmaj no xxmaj evil for a real slasher .,xxbos a friend once asked me to read a screenplay of his that had been xxunk by a movie studio . xxmaj to say it was one of the most inept and insipid scripts i 'd ever read would be a bold understatement . xxmaj yet i never told him this . xxmaj why ? xxmaj because in a world where films like \" xxmaj while xxmaj she xxmaj was xxmaj out \" can be green - xxunk and attract an xxmaj xxunk winning star like xxmaj kim xxmaj basinger , a screenplay lacking in character , content and common sense is no guarantee that it wo n't sell . \n",
              " \n",
              "  xxmaj as so many other reviewers have pointed out , \" xxmaj while xxmaj she xxmaj was xxmaj out \" is a dreadfully under - written xxmaj woman - in - xxmaj xxunk film that has xxunk housewife xxmaj basinger hunted by four unlikely xxunk on xxmaj christmas xxmaj eve . xxmaj every xxunk is legitimate , from the weak dialog and bad acting to the jaw - dropping lapses of logic , but xxmaj basinger is such an interesting actress and the premise is not without promise . xxmaj here are a couple of things that struck me : \n",
              " \n",
              "  1 ) i do n't care how much we are supposed to think her husband is a jerk , the house xxup is a mess with toys . xxmaj since when did it become child abuse to make kids pick up after themselves ? \n",
              " \n",
              "  2 ) xxmaj racially diverse gangs are rare everywhere except xxmaj hollywood , where they are usually the only racially balanced groups on screen . \n",
              " \n",
              "  3 ) xxmaj sure the film is stupid . xxmaj but so are the countless \" thrillers \" i 've sat through where the women are portrayed as xxunk , helpless victims of male sadism . xxmaj stupid or not , i found it refreshing to see a woman getting the best of her tormentors . \n",
              " \n",
              "  4 ) i xxup loved the ending ! \n",
              " \n",
              "  5 ) xxmaj though an earlier reviewer xxunk this phrase , i really xxup do think this film should be xxunk \" xxmaj the xxmaj red xxmaj xxunk of xxmaj doom . \",xxbos i had heard some not too good things about this movie and had probably seen the low score here at imdb and that 's why i had avoided it . xxmaj today they showed xxmaj vanilla xxmaj sky on xxup tv and as i had nothing better to do ... and as it turned out , i would have had a hard time finding anything better to do . xxmaj vanilla xxmaj sky is a frightening , sad and touching movie , actually one of the best i 've seen in a while . i was surprised by how i was affected watching it . xxmaj it 's hard to explain , but during the movie your feelings towards the characters and your perception of what is going on changes and it 's quite an emotional journey . xxmaj vanilla xxmaj sky really touched me in a way that is very rare for a movie , or any media for that matter . \n",
              " \n",
              "  i really recommend everyone to watch this movie . xxmaj regardless of what you have heard about it .,xxbos xxmaj without doubt xxmaj beat xxmaj street is the best film about the xxunk scene . xxmaj everything about it is spot on , the clothes ( xxunk music and most importantly the dancing ! xxmaj the storyline is basic , but hey what s there to tell a story about ? xxmaj the whole point of the film is to show what kids of that moment in time were doing , what xxunk to them . xxmaj it shows that teenagers in general are good , all that xxunk to these everyday kids was music , dancing and friendship . xxmaj having watched the xxup dvd recently i was xxunk surprised how well it had stood the test of time ! xxmaj the clothes did n't look dated ( possibly because xxmaj xxunk is now having a massive xxunk music still sounds fresh , and the dancing is still captivating to watch . a film anyone 10 - 25 years of age should see as part of their youth culture .,xxbos xxmaj in 17th xxmaj century xxmaj japan , there lived a samurai who would set the standard for the ages . xxmaj his name was xxmaj mayeda . xxmaj he is sent on an epic journey across the world to acquire 5,000 xxunk from the xxmaj king of xxmaj xxunk . xxmaj whilst at sea a violent storm xxunk their precious gold intended to buy the weapons and almost takes their lives . xxmaj mayeda must battle all odds to survive and the secure the fate of his beloved xxmaj japan . xxmaj shogun xxmaj mayeda is a multi million dollar action adventure epic set across three xxunk . \n",
              " \n",
              "  xxmaj starring cinema legends xxmaj sho xxmaj kosugi ( xxmaj tenchu : xxmaj stealth xxmaj assassins ) , xxmaj christopher xxmaj lee ( xxmaj star xxmaj wars , xxmaj lord of the xxmaj rings xxmaj trilogy ) , xxmaj john xxmaj rhys xxmaj davies ( xxmaj lord of the xxmaj rings xxmaj trilogy , xxmaj indiana xxmaj jones xxmaj trilogy ) and xxmaj xxunk xxmaj xxunk ( xxmaj the xxmaj seven xxmaj samurai , xxmaj xxunk of xxmaj blood ) , xxmaj shogun xxmaj mayeda ( xxmaj xxunk ) is a film masterpiece . \n",
              " \n",
              "  xxmaj the xxmaj xxunk winning stunt team bought to the screen by xxmaj bob xxmaj xxunk bring exciting battle / action sequences such as the opening battle against the xxmaj eastern army , the attack on xxmaj mayeda 's ship carrying priceless xxunk and the final confrontation between xxmaj sho xxmaj kosugi and xxmaj david xxmaj xxunk . a fine musical score by famed xxunk xxmaj john xxmaj scott is also present . xxmaj director xxmaj sho xxmaj xxunk was not even nominated for an xxmaj oscar for this film which deals with the emotional strife such as the death of xxmaj xxunk family , the search for love and acceptance after xxunk , all of which is dealt with extremely well . xxmaj highly recommended cinematic masterpiece . \n",
              " \n",
              "  xxmaj please note : xxmaj all of the above is opposite for the film in question .\n",
              "y: CategoryList\n",
              "negative,negative,positive,positive,negative\n",
              "Path: /root/.fastai/data/imdb_sample;\n",
              "\n",
              "Valid: LabelList (201 items)\n",
              "x: TextList\n",
              "xxbos xxmaj if you ever see a stand up comedy movie this is the one . xxmaj you will laugh nonstop if you have any sense of humor at all . xxmaj this is a once in a lifetime performance from a once in a lifetime performer . xxmaj this is a stand up standard .,xxbos xxmaj of the three titles from xxmaj xxunk xxmaj franco to find their way onto the xxmaj official xxup xxunk xxmaj video xxmaj nasty list ( xxmaj devil xxmaj hunter , xxmaj bloody xxmaj moon and xxmaj women xxmaj behind xxmaj bars ) this is perhaps the least deserving of xxunk , being a dreadfully dull jungle clunker enlivened only very slightly by a little inept gore , a gratuitous rape scene , and loads of nudity . \n",
              " \n",
              "  xxmaj gorgeous blonde xxmaj xxunk xxmaj buchfellner plays movie star xxmaj laura xxmaj xxunk who is abducted by a gang of ruthless kidnappers and taken to a remote xxunk island inhabited by a savage xxunk who worship the ' devil god ' that lurks in the jungle ( a big , naked , xxunk - xxunk native who likes to eat the hearts of xxunk female sacrifices ) . \n",
              " \n",
              "  xxmaj employed by xxmaj laura 's agent to deliver a $ xxunk ransom , brave xxunk xxmaj peter xxmaj xxunk ( xxmaj al xxmaj xxunk ) and his xxmaj vietnam vet pilot pal travel to the island , but encounter trouble when the bad guys attempt a double - cross . xxmaj during the confusion , xxmaj laura escapes into the jungle , but runs straight into the arms of the island 's natives , who offer her up to their god . \n",
              " \n",
              "  xxmaj franco directs in his usual xxunk style and loads this laughable effort with his usual dreadful trademarks : crap gore , murky cinematography , rapid zooms , numerous crotch shots , out of focus imagery , awful sound effects , and ham - fisted editing . xxmaj the result is a dire mess that is a real struggle to sit through from start to finish ( xxmaj it took me a couple of xxunk to finish the thing ) , and even the sight of the xxunk xxmaj buchfellner in all of her natural glory ai n't enough to make me xxunk this film in a hurry .,xxbos xxmaj this movie succeeds at being one of the most unique movies you 've seen . xxmaj however this comes from the fact that you ca n't make heads or tails of this mess . xxmaj it almost seems as a series of challenges set up to determine whether or not you are willing to walk out of the movie and give up the money you just paid . xxmaj if you do n't want to feel xxunk you 'll sit through this horrible film and develop a real sense of pity for the actors involved , they 've all seen better days , but then you realize they actually got paid quite a bit of money to do this and you 'll lose pity for them just like you 've already done for the film . i ca n't go on enough about this horrible movie , its almost something that xxmaj ed xxmaj wood would have made and in that case it surely would have been his masterpiece . \n",
              " \n",
              "  xxmaj to start you are forced to sit through an opening dialogue the likes of which you 've never seen / heard , this thing has got to be five minutes long . xxmaj on top of that it is narrated , as to suggest that you the viewer can not read . xxmaj then we meet xxmaj mr. xxmaj xxunk and the xxunk of terrible lines gets xxunk , it is as if he is operating solely to get lines on to the movie poster tag line . xxmaj soon we meet xxmaj stephen xxmaj xxunk , who i typically enjoy ) and he does his best not to drown in this but ultimately he does . xxmaj then comes the ultimate insult , xxmaj tara xxmaj reid playing an intelligent role , oh help us ! xxmaj tara xxmaj reid is not a very talented actress and somehow she xxunk gets roles in movies , in my opinion though she should stick to movies of the xxmaj american pie type . \n",
              " \n",
              "  xxmaj all in all you just may want to see this for yourself when it comes out on video , i know that i got a kick out of it , i mean lets all be honest here , sometimes its comforting to xxunk in the shortcomings of others .,xxbos xxmaj this has got to be the worst horror movie i have ever seen . i remember watching it years ago when it initially came out on video and for some strange reason i thought i enjoyed it . xxmaj so , like an idiot , i ran out to purchase the xxup dvd once it was released ... what a tragic mistake ! i wo n't even bother to go into the plot because it is so transparent that you can see right through it anyhow . i am a fan of xxmaj xxunk xxmaj gordon xxmaj lewis so i am xxunk to cheesy gore effects and bad acting but these people take this to a whole different level . xxmaj it is almost as if they are intentionally trying to make the worst movie xxunk possible ... if that was their goal , they xxunk . xxmaj if they intended to make a film that was supposed to scare you or make you believe in any way , shape , or form that it is real then they failed ... xxup miserably ! xxmaj avoid this movie ... read the plot synopsis and you 've seen it !,xxbos xxmaj he now has a name , an identity , some memories and a a lost girlfriend . xxmaj all he wanted was to disappear , but still , they traced him and destroyed the world he hardly built . xxmaj now he wants some explanation , and to get ride of the people how made him what he is . xxmaj yeah , xxmaj jason xxmaj bourne is back , and this time , he 's here with a vengeance . \n",
              " \n",
              "  xxup ok , this movie does n't have the most xxunk script in the world , but its thematics are very clever and ask some serious questions about our society . xxmaj of course , like every xxmaj xxunk movie since the end of the 90 's , \" xxmaj the xxmaj bourne xxmaj suprematy \" is a super - heroes story . xxmaj jason xxmaj bourne is a xxmaj captain - xxmaj america project - like , who 's gone completely wrong . xxmaj in the first movie , the hero discovered his abilities and he accepted them in the second one . xxmaj he now fights against what he considers like evil , after a person close to him has been killed ( his girlfriend in \" xxmaj suprematy \" ) by them . xxmaj that 's all a part of the super - hero story , including a character with ( realistic but still impressive : he almost invincible ) super powers . \n",
              " \n",
              "  xxmaj and the interesting point is that the evil he fights all across the world ( there 's no xxunk in the xxmaj bourne 's movies , characters are going from one continent to another in the blink of an eye ) , is , as in the best seasons of \" 24 \" , an xxmaj american enemy , who 's beliefs that he fight for the good of his country completely blinds him . xxmaj funny how \" mad patriots \" are now the xxup xxunk enemies of xxunk xxmaj hollywood 's stories . \n",
              " \n",
              "  xxmaj beside all those interesting thematics , the movie is n't flawless : the feminine character of xxmaj xxunk xxmaj xxunk is for now on completely useless and the direction is quite xxunk when it comes to dialogs scenes . xxmaj but all that does n't really matter , for \" xxmaj the xxmaj bourne xxmaj ultimatum \" is an action movie . xxmaj and the action scenes are rather impressive . \n",
              " \n",
              "  xxmaj everyone here is talking about the \" xxmaj xxunk scene \" and the \" xxmaj tanger pursuit \" and everyone 's right . i particularly enjoyed the fight in xxmaj tanger , that reminds my in its exaggeration and xxunk the works of xxmaj xxunk xxmaj xxunk . xxmaj visually inventive scenes , lots of intelligent action parts and a good reflection on xxmaj american 's contemporary thematics : \" xxmaj the xxmaj bourne xxmaj ultimatum \" is definitely the best movie of the series and a very interesting and original action flick .\n",
              "y: CategoryList\n",
              "positive,negative,negative,negative,positive\n",
              "Path: /root/.fastai/data/imdb_sample;\n",
              "\n",
              "Test: None, model=SequentialRNN(\n",
              "  (0): MultiBatchEncoder(\n",
              "    (module): AWD_LSTM(\n",
              "      (encoder): Embedding(8904, 400, padding_idx=1)\n",
              "      (encoder_dp): EmbeddingDropout(\n",
              "        (emb): Embedding(8904, 400, padding_idx=1)\n",
              "      )\n",
              "      (rnns): ModuleList(\n",
              "        (0): WeightDropout(\n",
              "          (module): LSTM(400, 1152, batch_first=True)\n",
              "        )\n",
              "        (1): WeightDropout(\n",
              "          (module): LSTM(1152, 1152, batch_first=True)\n",
              "        )\n",
              "        (2): WeightDropout(\n",
              "          (module): LSTM(1152, 400, batch_first=True)\n",
              "        )\n",
              "      )\n",
              "      (input_dp): RNNDropout()\n",
              "      (hidden_dps): ModuleList(\n",
              "        (0): RNNDropout()\n",
              "        (1): RNNDropout()\n",
              "        (2): RNNDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fd5c4f5a3b0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/imdb_sample'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
              "learn: ...\n",
              "alpha: 2.0\n",
              "beta: 1.0, MixedPrecision\n",
              "learn: ...\n",
              "loss_scale: 65536\n",
              "max_noskip: 1000\n",
              "dynamic: True\n",
              "clip: None\n",
              "flat_master: False\n",
              "max_scale: 16777216\n",
              "loss_fp32: True], layer_groups=[Sequential(\n",
              "  (0): Embedding(8904, 400, padding_idx=1)\n",
              "  (1): EmbeddingDropout(\n",
              "    (emb): Embedding(8904, 400, padding_idx=1)\n",
              "  )\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(400, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 1152, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): WeightDropout(\n",
              "    (module): LSTM(1152, 400, batch_first=True)\n",
              "  )\n",
              "  (1): RNNDropout()\n",
              "), Sequential(\n",
              "  (0): PoolingLinearClassifier(\n",
              "    (layers): Sequential(\n",
              "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): Dropout(p=0.1, inplace=False)\n",
              "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")], add_time=True, silent=False)"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZnwUycBOEpc"
      },
      "source": [
        "# data_clas.show_batch()"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2b_Yy-vOPCs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "c0bdcd9f-0486-4c10-969e-6f1ee81f7476"
      },
      "source": [
        "learn_c.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.632630</td>\n",
              "      <td>0.594487</td>\n",
              "      <td>0.716418</td>\n",
              "      <td>00:16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUaHSekaVa3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "439382c8-08a9-45f4-8354-af0fdc074f27"
      },
      "source": [
        "learn_c.unfreeze()\n",
        "learn_c.fit_one_cycle(3, slice(1e-4, 1e-2))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.474546</td>\n",
              "      <td>0.437509</td>\n",
              "      <td>0.865672</td>\n",
              "      <td>00:43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.413244</td>\n",
              "      <td>0.329415</td>\n",
              "      <td>0.870647</td>\n",
              "      <td>00:45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.339283</td>\n",
              "      <td>0.319156</td>\n",
              "      <td>0.860696</td>\n",
              "      <td>00:41</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4ZGWEWcYlrt"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsFQRbrPP6Ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12cc1bb7-db19-4d27-d810-cc7896aeef28"
      },
      "source": [
        "review = \"This was a ok.\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")\n"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive | 1: tensor([0.2850, 0.7150])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqiDuqXZlyFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874fe312-0ba8-4b41-a6f3-ee9e5b2cd712"
      },
      "source": [
        "review = \"I didn't like the movie\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative | 0: tensor([0.8313, 0.1687])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0UMGriIl345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c39191-1911-4038-a5ef-35b02f640f38"
      },
      "source": [
        "review = \"The moive did not reach my expectation\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")\n"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive | 1: tensor([0.4976, 0.5024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dlRFVI2l9vm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7ce8a9-0541-4fc8-8488-891ef81ad6b6"
      },
      "source": [
        "review = \"audience were laughing and screaming throughout\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive | 1: tensor([0.3570, 0.6430])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SAkXWummB61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a05614e-9d97-48af-8c38-fa7f41a75fff"
      },
      "source": [
        "review = \"the movie was bad!\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive | 1: tensor([0.4867, 0.5133])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx7XNG20mTvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f4ddea-ef94-4915-a3e8-efe1aba8308f"
      },
      "source": [
        "review = \"This plot of the movie was confusing which left the audience bored\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")\n"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative | 0: tensor([0.9008, 0.0992])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN_A5IETZU3z",
        "outputId": "cd32cea8-bcae-4cf1-a513-c02c0175efba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# after re-training run the above predictions again\n",
        "learn_c.fit_one_cycle(3, slice(1e-4, 1e-2))"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.221002</td>\n",
              "      <td>0.406384</td>\n",
              "      <td>0.815920</td>\n",
              "      <td>00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.216234</td>\n",
              "      <td>0.358415</td>\n",
              "      <td>0.855721</td>\n",
              "      <td>00:43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.178552</td>\n",
              "      <td>0.321629</td>\n",
              "      <td>0.880597</td>\n",
              "      <td>00:45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1702: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  if p.grad is not None:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcmS-ZqcaezM",
        "outputId": "285173cc-bd10-4df2-97be-d491d84795a6"
      },
      "source": [
        "review = \"This was a ok.\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")\n"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive | 1: tensor([0.0816, 0.9184])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYWQ7ad7aezV",
        "outputId": "98f08809-8dd0-4b90-b2e1-683f010d3485"
      },
      "source": [
        "review = \"I didn't like the movie\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive | 1: tensor([0.4836, 0.5164])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axs7ocz3aezW",
        "outputId": "54295ede-edfb-493f-af82-3ebfee61ca49"
      },
      "source": [
        "review = \"The moive did not reach my expectation\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")\n"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive | 1: tensor([0.3270, 0.6730])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goNZt-nwaezW",
        "outputId": "05eb203e-fd66-4370-b6e3-6c70815496c7"
      },
      "source": [
        "review = \"audience were laughing and screaming throughout\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive | 1: tensor([0.1803, 0.8197])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InJV37goaezW",
        "outputId": "2017d79c-63d1-4504-8c05-21cb17610c2a"
      },
      "source": [
        "review = \"the movie was bad!\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive | 1: tensor([0.0694, 0.9306])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBMGHvhtaezW",
        "outputId": "e741b154-3ac9-43a5-f08a-d61af27ec29f"
      },
      "source": [
        "review = \"This plot of the movie was confusing which left the audience bored\"\n",
        "pred_cls, pred_idx, confidence_score = learn_c.predict(review)\n",
        "print(f\"{pred_cls.obj} | {pred_idx}: {confidence_score}\")\n"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative | 0: tensor([0.9979, 0.0021])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8dw9y7YZErX"
      },
      "source": [
        "## Analysis of the Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4HJOtKEn0BH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ddf4ce-a95e-457e-fe9e-23712dfe3b16"
      },
      "source": [
        "print(learn_c.data.classes) # to check the classes order considered "
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['negative', 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5ALg_yxJQ77",
        "outputId": "7db9acc3-41b3-472b-c911-e6bfbc5538fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "preds = learn.get_preds()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDYixIoJnxbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988c5cc6-3576-412c-ce27-345fbe601965"
      },
      "source": [
        "predictions = np.argmax(preds[0], axis = 1) \n",
        "targets = preds[1]\n",
        "print (f\"predict:{predictions}\")\n",
        "print(f\"target:{targets}\")\n",
        "confusion_matrix = metrics.confusion_matrix(targets, predictions)\n",
        "print(confusion_matrix)\n",
        "target_names = learn_c.data.classes\n",
        "print(metrics.classification_report(targets, predictions, target_names=target_names))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict:tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
            "        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
            "        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
            "        1, 0, 1, 1, 0, 0, 0, 1, 0])\n",
            "target:tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
            "        0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
            "        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
            "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
            "        1, 0, 1, 0, 0, 0, 0, 1, 0])\n",
            "[[94 19]\n",
            " [ 8 80]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.92      0.83      0.87       113\n",
            "    positive       0.81      0.91      0.86        88\n",
            "\n",
            "    accuracy                           0.87       201\n",
            "   macro avg       0.86      0.87      0.87       201\n",
            "weighted avg       0.87      0.87      0.87       201\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWyl-8RrvTHY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adee5c47-eed3-4d6d-c2e4-e12d2a64b8e3"
      },
      "source": [
        "#Plot the error curves\n",
        "learn_c.recorder.plot()\n",
        "learn_c.recorder.plot_losses()\n",
        "learn_c.recorder.plot_lr()\n",
        "learn_c.recorder.plot_metrics()"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8dcnCSHsa4CQAImAIMhqxCV1AUQR3KuCrbdK22tv79XutdrbX2u9t9W292rbe3vvrbWCS6sEt1KlpQq4sWiGfRNECEzCFpB9J3x+f8zBjjjAAJnMZOb9fDzyyDlnzpn5HEjymfM9c97H3B0REZFjZSW7ABERSU1qECIiEpMahIiIxKQGISIiMalBiIhITDnJLqCutG/f3ouLi5NdhohIgzJ37twt7p4f67G0aRDFxcWEQqFklyEi0qCY2drjPaYhJhERiUkNQkREYlKDEBGRmNQgREQkJjUIERGJSQ1CRERiUoMQEZGY0uY6CJFUNW35Jszg8rM7kJVlyS5HJG5qECIJtHX3Af7pmbkcqnWK2zXljouLufm8IlrkNUp2aSInpSEmkQR6aX41h2qd74/qTdtmufz4z8u46KHp/PjPS1m7dU+yyxM5IR1BiCSIu1MeCjOgS2vuurQ7d13anYXh7YyfuYZn5qxlwqxKhvXqwLiyEsp6tMNMw0+SWnQEIZIgC6t2sHLTbsaUdvl42YAurfnl2EHM/N4w7hnagwXh7dz++3e56pdv8cd317HvYG0SKxb5JDUIkQSZWBEmr1EW1wwo+NRjHVrm8a0rezHzvmH84ub+5GRl8f2XFnPhQ9N46C/Lqd6+LwkVi3yShphEEmDfwVr+vHA9o/oV0PIEJ6TzGmVzS2kXbj6viIrKbYyfuYbfvbWax99ew1V9OzKurITSbm00/CRJoQYhkgBTFm9g94HD3Bo1vHQiZsaQkrYMKWlL1ba9PD17Lc++t44pizdybmFLxl1cwjUDCmick53gykX+ztw92TXUidLSUtf9ICRV3Prb2WzeuZ8Z37n8tN/97z14mJfmVzN+ZiWrNu+mffNcPndBN26/sCsdWuTVccWSqcxsrruXxnpM5yBE6tiaLXt4b81H3FLa5YyGhprm5vD5C7rx2jcv5ekvDaF/UWt+Pe0Dyh6ezjcnLmBR1fY6rFrk0zTEJFLHJoXCZBl8dnBRnTyfmXFJz3wu6ZnPmi17eHJWJZNCYV6aX8153dowrqyYq/p2olG23u9J3dIQk0gdOlx7hLKfTadPQUvGjxuSsNfZuf8Qz4eqeHJ2JWu37qWgVR63X9iNzw3pSptmuQl7XUk/GmISqSdvfVDDpp0HGHN+fCenT1fLvEZ88TMlTP/25Tz+hVLOym/GL6au4MKHpnHfC4t4f+POhL6+ZAYNMYnUofKKKto1y2VY74718nrZWcYVfTpyRZ+OrNi4iwmz1vDivGqeqwhzcfd2jCsrYVjvDmQrJFBOg44gROrIlt0HeH35Jm4cVEhuTv3/avXq1IKHburPnPuH872RvVmzZQ//+FSIof/xBo+/vZqd+w/Ve03SsKlBiNSRl+dXc/iIc2uCh5dOpk2zXL56eXfevncov/ncYDq0aMy/v7qci346jR/9aQmra3YntT5pODTEJFIH3J2JFWEGdmnN2R1bJLscAHKysxjdv4DR/QtYXLWD8TPX8Mf31vHk7LVc3iufcWUlXNqzva7SluPSEYRIHVgQ3s4Hm3cn/OT06epX1IpHxgxk5n3D+MYVPVlSvZM7nniPKx55k6fnrGXvwcPJLlFSUEIbhJmNNLMVZrbKzO6L8XhXM5thZvPNbJGZjQqWjzCzuWa2OPg+LJF1ipyp8lCYJo2yuab/p4P5UkmHFnl844qzmXnfUB65dQBNc3P4fy8v4cKfTuMnry4j/NHeZJcoKSRhQ0xmlg38BhgBVAEVZjbZ3ZdFrfYDoNzd/9fM+gBTgGJgC3Ctu683s3OBqUBhomoVORN7Dx7mzws3MKpfQYO5U1zjnGxuGlzEjYMKmbt2G+NnVfLEzEp+/84aRvSJhAReUNJWw08ZLpHnIIYAq9x9NYCZPQdcD0Q3CAdaBtOtgPUA7j4/ap2lQBMza+zuBxJYr8hpmbJ4YxDMVzdXTtcnM6O0uC2lxW1Zv30fT8+JhAROXbqJcwpaMq6smOsGdCavkUICM1Eih5gKgXDUfBWfPgp4ALjdzKqIHD3cE+N5PgvMi9UczOwuMwuZWaimpqZuqhY5ReUVYYrbNWVISdtkl3JGOrduwvdG9mb2fcN56KZ+1B45wr3PL6Ls4en8599WsGnn/mSXKPUs2SepbwMmuHsRMAp42sw+rsnM+gI/A74Sa2N3f8zdS929ND8/v14KFom2umY371WeeTBfKmmSm81tQ7oy9RuX8ocvX8Cgrq357xmrKHt4Ol97dj7z121LdolSTxI5xFQNRH+koyhYFu1LwEgAd59tZnlAe2CzmRUBLwFfcPcPE1inyGmbNLeKLIObz2t4w0snY2aU9WhPWY/2rN26hydnrWVSKMzkhesZ2KU148qKGdWvQCGBaSyR/7MVQE8zKzGzXGAsMPmYddYBwwHM7BwgD6gxs9bAq8B97j4zgTWKnLbDtUd4YW4VQ3t1oGPL9L4/Q7d2zfjhtX2Y/f3hPHBtH3bsO8TXn1vAZ342nf+a9gFbd+v0YDpKWINw98PA3UQ+gbScyKeVlprZg2Z2XbDat4F/NLOFwLPAnR6Jl70b6AH80MwWBF8dElWryOl4c2UNm3cd4JY47xqXDpo3zuHOshKmfesyxt95Pmd3bMF/vraSix6ezncnLWTZeoUEphPFfYucpq88HWLu2m3Mvn94Rg+zfLBpFxNmVfLivGr2HarlgpK2jCsrZkSfTgoJbAAU9y1Sx2p2HWDa8s3cOKgwo5sDQM+OLfjJjf2Yc/9w7r+6N1Xb9vFPz8zj0p/P4LG3PmTHXoUENlSZ/ZMtcpo+DubLoOGlk2nVtBFfuaw7b373cv7v9sEUtmnCT6e8z4UPTeMHLy9m1WaFBDY0CusTOUXuzsRQmEFdW9MzRYL5UklOdhYjzy1g5LkFLKnewYRZlZRXVPHMnHVc0rM9Xywr4bKz88nS8FPK0xGEyCmaH97Oqs27GaOjh5M6t7AV/3HLAGbdP4xvjTib9zfuYtyECq545E2enFXJ7gMKCUxlahAip6i8IhLMNzrFg/lSSfvmjfna8J7M/N4wfjV2IC2aNOJHk5dy0U+n8W+vLGPdVoUEpiINMYmcgkgw33pG9284wXypJDcni+sHFnL9wELmrdvG+JmVPDmrkidmrmF47458sayYi7q3S5ur0hs6NQiRU/Dqog3sOVirk9N1YHDXNgzu2oaNo87hmTlr+eN763h9+SZ6d2rBhWe1Qz0ifoWtm/DlS86q8+dVgxA5BeWhMCXtm3F+cZtkl5I2OrXK4ztX9eLuYT2YvGA9T82p5IV5Vckuq0HpX9RKDUIkmVbX7Kaichv3juylIZAEyGuUza3nd0n6Pb3l73SSWiRO5aEqsrOMmwenXzCfSCxqECJxOFx7hBfmVTG0Vz4d0jyYT+QoNQiROLyxooaaDAvmE1GDEIlDeShM++a5DOutUGHJHGoQIidRs+sA09/fzE2DizI+mE8yi37aRU7ipflVQTCfTk5LZlGDEDkBd2diRZjBXVvTo4OC+SSzqEGInMC8ddv5sGYPY/TZfMlAahAiJ1BeEaZpbjaj+3dOdiki9U4NQuQ49hw4zCuL1jO6XwHNGyt0QDKPGoTIcby6OAjm0/CSZCg1CJHjKK8Ic1b7ZpR2UzCfZCY1CJEYPqzZTWjtNm4p7aJgPslYahAiMZSHwmRnGZ89rzDZpYgkjRqEyDEO1R7hhbnVDO3VgQ4tFMwnmSuhDcLMRprZCjNbZWb3xXi8q5nNMLP5ZrbIzEYFy9sFy3eb2X8nskaRY72xooYtuw/oymnJeAlrEGaWDfwGuBroA9xmZn2OWe0HQLm7DwLGAv8TLN8P/D/gO4mqT+R4IsF8jRmqYD7JcIk8ghgCrHL31e5+EHgOuP6YdRxoGUy3AtYDuPsed3+HSKMQqTebd+1n+vub+ezgQgXzScZL5G9AIRCOmq8KlkV7ALjdzKqAKcA9p/ICZnaXmYXMLFRTU3MmtYoA8NK8amqPuO77IELyT1LfBkxw9yJgFPC0mcVdk7s/5u6l7l6an5+fsCIlM7g7E0NhzuvWhh4dmie7HJGkS2SDqAai34YVBcuifQkoB3D32UAe0D6BNYkc17x121hds4cxOnoQARLbICqAnmZWYma5RE5CTz5mnXXAcAAzO4dIg9BYkSTFxCCYb1T/gmSXIpISEpZA5u6HzexuYCqQDTzh7kvN7EEg5O6TgW8DvzOzbxI5YX2nuzuAmVUSOYGda2Y3AFe6+7JE1SuZbfeBw7yyaAPX9Fcwn8hRCf1NcPcpRE4+Ry/7YdT0MqDsONsWJ7I2kWhTFm1g78FabtXwksjHkn2SWiQlTAyFOSu/GecpmE/kY2oQkvFWbd7N3LXbuFXBfCKfoAYhGW9SEMx302AF84lEU4OQjHao9ggvzKtmWG8F84kcSw1CMtqM9zcHwXw6OS1yLDUIyWjloSryWzRmaC9diS9yLDUIyVibd+5nxorN3DS4kBwF84l8in4rJGO9OD8SzKfhJZHY1CAkI7k75RVhSru1oXu+gvlEYlGDkIw0d+02Vm/Zw63n6+hB5HjUICQjTawI0yw3m9H9FMwncjxqEJJxdh84zKuLN3BN/840UzCfyHGpQUjGeXXR+kgw3/lFyS5FJKWpQUjGmVgRpnt+MwZ3VTCfyImoQUhGWbV5F/PWbWfM+QrmEzkZNQjJKOWhKnKyjBsHaXhJ5GTUICRjHKo9wovzqhjWuwP5LRonuxyRlKcGIRlj+vub2bL7oK6cFomTGoRkjEmhMPktGnO5gvlE4qIGIRkhEsxXw2cHFymYTyROcf2mmNmLZjbazPSbJQ3SC/OOBvPp5LRIvOL9g/8/wOeAD8zsYTPrlcCaROqUuzMpFOb84jacpWA+kbjF1SDc/XV3/zwwGKgEXjezWWY2zswaJbJAkTMVOhrMp5PTIqck7iEjM2sH3Al8GZgP/IpIw3gtIZWJ1JGjwXyjFMwnckriPQfxEvA20BS41t2vc/eJ7n4PcNxjdjMbaWYrzGyVmd0X4/GuZjbDzOab2SIzGxX12P3BdivM7KpT3zUR2LX/EK8u2sC1AxTMJ3Kq4v2N+bW7z4j1gLuXxlpuZtnAb4ARQBVQYWaT3X1Z1Go/AMrd/X/NrA8wBSgOpscCfYHORIa0znb32jjrFQHg1UUb2Heolls0vCRyyuIdYupjZq2PzphZGzP755NsMwRY5e6r3f0g8Bxw/THrONAymG4FrA+mrweec/cD7r4GWBU8n8gpmRgK06NDcwZ3bX3ylUXkE+JtEP/o7tuPzrj7NuAfT7JNIRCOmq8KlkV7ALjdzKqIHD3ccwrbYmZ3mVnIzEI1NTXx7IdkkA827WL+uu2MKVUwn8jpiLdBZFvUb1gwfJRbB69/GzDB3YuAUcDTp3Kthbs/5u6l7l6an6+rY+WTykPhSDDf4E+9txCROMR7DuKvwEQz+20w/5Vg2YlUA9EDv0XBsmhfAkYCuPtsM8sD2se5rchxRYL5qhl+TgfaN1cwn8jpiPfd+veAGcBXg69pwL0n2aYC6GlmJWaWS+Sk8+Rj1lkHDAcws3OAPKAmWG+smTU2sxKgJ/BenLWKMG35ZrbuUTCfyJmI6wjC3Y8A/xt8xcXdD5vZ3cBUIBt4wt2XmtmDQMjdJwPfBn5nZt8kcsL6Tnd3YKmZlQPLgMPAv+gTTHIqJoXCdGjRmMvO1tCjyOmKq0GYWU/gIaAPkXf5ALj7WSfazt2nEDn5HL3sh1HTy4Cy42z7E+An8dQnEm3Tzv3MWLGZr1zWXcF8Imcg3t+e8USOHg4DQ4GngGcSVZTImXhhXhVHHA0viZyheBtEE3efBpi7r3X3B4DRiStL5PREgvmqGFLclpL2zZJdjkiDFu+nmA4EHz/9IDivUM0JIjZEkqWichtrtuzhX4b2SHYpIg1evEcQXyeSw/Q14DzgduCORBUlcromVoRp3jiHUf06JbsUkQbvpEcQwUVxY9z9O8BuYFzCqxI5Dbv2H2LK4g3cMKgzTXMVzCdypk56BBF8vPQz9VCLyBl5RcF8InUq3rdZ881sMjAJ2HN0obu/mJCqRE7DxIowPTs0Z1AXBfOJ1IV4G0QesBUYFrXMATUISQkrN+1iQXg7Pxh9joL5ROpIvFdS67yDpLTyikgw3w2DFMwnUlfivZJ6PJEjhk9w9y/WeUUip+jg4SO8NL+aK87pqGA+kToU7xDTK1HTecCN/P3mPiJJNf39TZFgvvOLkl2KSFqJd4jpheh5M3sWeCchFYmcovJQFR1bNubSngrmE6lLp5tk1hPoUJeFiJyOjTv288aKzXx2cJGC+UTqWLznIHbxyXMQG4ncI0IkqRTMJ5I48Q4xtUh0ISKnKhLMF2ZISVuKFcwnUufiOiY3sxvNrFXUfGszuyFxZYmc3LtrPqJy617G6OhBJCHiHbT9kbvvODrj7tuBHyWmJJH4lIciwXxXK5hPJCHibRCx1lMamiTNziCY79oBCuYTSZR4G0TIzB4xs+7B1yPA3EQWJnIiryzcwP5DR7i1VNc+iCRKvA3iHuAgMBF4DtgP/EuiihI5mYmhMGd3bM5ABfOJJEy8n2LaA9yX4FpE4rJi4y4WKphPJOHi/RTTa2bWOmq+jZlNTVxZIsdXHgrTKNu4UcF8IgkV7xBT++CTSwC4+zZ0JbUkQXQwXzsF84kkVLwN4oiZdT06Y2bFxEh3FUm0acs38dGeg7pyWqQexNsg/hV4x8yeNrNngDeB+0+2kZmNNLMVZrbKzD51DsPMHjWzBcHXSjPbHvXYz8xsSfA1Jt4dkvRWHgrTqWUel56tYD6RRIv3JPVfzawUuAuYD7wM7DvRNmaWDfwGGAFUARVmNtndl0U97zej1r8HGBRMjwYGAwOBxsAbZvYXd995CvsmaWbjjv28ubKGr17enewsnZwWSbR4T1J/GZgGfBv4DvA08MBJNhsCrHL31e5+kMjHY68/wfq3Ac8G032At9z9cPAJqkXAyHhqlfR1NJjvlvM0vCRSH+IdYvo6cD6w1t2HEnmnv/3Em1AIhKPmq4Jln2Jm3YASYHqwaCEw0syamll7YCjwqb8KZnaXmYXMLFRTUxPnrkhDdOSIUx4Kc4GC+UTqTbwNYr+77wcws8bu/j7Qqw7rGAs87+61AO7+N2AKMIvIUcVsoPbYjdz9MXcvdffS/HyNSaezd9d8xNqtexlzvo4eROpLvA2iKrgO4mXgNTP7E7D2JNtU88l3/UXBsljG8vfhJQDc/SfuPtDdRwAGrIyzVklDk0JhWjTO4epzC5JdikjGiPck9Y3B5ANmNgNoBfz1JJtVAD3NrIRIYxgLfO7YlcysN9CGyFHC0WXZQGt332pm/YH+wN/iqVXSz879h5iyZAM3DS6iSW52sssRyRinHIPp7m/Gud5hM7sbmApkA0+4+1IzexAIufvkYNWxwHPuHn1dRSPg7SBGYSdwu7sfPtVaJT38eeF69h86ovs+iNSzhOYku/sUIucSopf98Jj5B2Jst5/IJ5lEKK8I06tjC/oXtTr5yiJSZ3SXd0lp72/cycKqHdx6fhcF84nUMzUISWnlFVUK5hNJEjUISVmRYL4qRvTpSNtmuckuRyTjqEFIynp9+Sa27T3ELTo5LZIUahCSsj4O5uupiyBFkkENQlLShh37eGtlDTefV6RgPpEkUYOQlPTC3CCYr7Qo2aWIZCw1CEk5kWC+Ki48qy3d2imYTyRZ1CAk5cxZs5V1HymYTyTZ1CAk5UwKVdGicQ4j+yqYTySZ1CAkpezYd4gpizdw3cDOCuYTSTI1CEkpf164ngOHj2h4SSQFqEFISikPhendqQX9ChXMJ5JsahCSMpZv2Mmiqh3cWqpgPpFUoAYhKaM8FKZRtnGDgvlEUoIahKSEA4dreXl+NVf26aRgPpEUoQYhKeH1ZZuDYD5dOS2SKtQgJCWUh8IUtMrjEgXziaQMNQhJuvXb9/HWBwrmE0k1ahCSdM/PrcIdbjlP1z6IpBI1CEmqI0ecSXPDXHRWO7q2a5rsckQkihqEJNWc1VsJf7RPV06LpCA1CEmq8lCYFnk5jDy3U7JLEZFjqEFI0uzYd4i/LNnI9QM7k9dIwXwiqSahDcLMRprZCjNbZWb3xXj8UTNbEHytNLPtUY/93MyWmtlyM/u1KXsh7Uw+GsxX2jXZpYhIDDmJemIzywZ+A4wAqoAKM5vs7suOruPu34xa/x5gUDB9MVAG9A8efge4DHgjUfVK/SuviATznVvYMtmliEgMiTyCGAKscvfV7n4QeA64/gTr3wY8G0w7kAfkAo2BRsCmBNYq9WzZ+p0srt7BmPMVzCeSqhLZIAqBcNR8VbDsU8ysG1ACTAdw99nADGBD8DXV3ZfH2O4uMwuZWaimpqaOy5dEKg+Fyc3O4oaBCuYTSVWpcpJ6LPC8u9cCmFkP4BygiEhTGWZmlxy7kbs/5u6l7l6an6+IhobiwOFaXl5QzYi+HWmjYD6RlJXIBlENRH+4vShYFstY/j68BHAjMMfdd7v7buAvwEUJqVLq3WvLNrF97yFuLdW1DyKpLJENogLoaWYlZpZLpAlMPnYlM+sNtAFmRy1eB1xmZjlm1ojICepPDTFJw1QeqqJzqzw+06N9sksRkRNIWINw98PA3cBUIn/cy919qZk9aGbXRa06FnjO3T1q2fPAh8BiYCGw0N3/nKhapf5Ub9/H2wrmE2kQEvYxVwB3nwJMOWbZD4+ZfyDGdrXAVxJZmyTH86EgmE/DSyIpL1VOUksGOBrMd3H3dnRpq2A+kVSnBiH1ZvbqrVRtUzCfSEOhBiH1pjwUpmVeDlf1VTCfSEOgBiH1Ysfeo8F8hQrmE2kg1CCkXkxeWM3Bw0c0vCTSgKhBAJ/8hK0kwsRQmHMKWtK3s4L5RBqKjG8Q+w/VcvWv3ubX0z5gy+4DyS4nLS1dv4Ml1TsZU1qkYD6RBiTjG8S2vQfp2DKPR15bycUPTec7kxaypHpHsstKK5NCVeRmZ3G9gvlEGpSEXijXEBS0asKTXxzCqs27eXJWJS/Mq+L5uVUMKW7LuLJiRvTpSE52xvfR07b/UC0vza/mSgXziTQ4+ssX6NGhOf92w7nMvn84Pxh9Dut37OOrf5jHZb94g9+++SE79h5KdokN0mvLNrFjn4L5RBoiS5cTtKWlpR4Khers+WqPOK8v38T4mWuYs/ojmjTK5sbBhYy7uJieHVvU2euku3/4/busrtnDW/cOVfaSSAoys7nuXhrrsYwfYjqe7Czjqr6duKpvJ5Zv2MmEmZU8P7eKP767jkt6tmdcWTGXn92BLP3RO66qbXt5Z9UW7hnWU81BpAHSEFMcziloyc9u7s+c+4fz3at6sXLTLr44IcTwR95kwsw17D5wONklpqTn5wbBfOcVJbsUETkNGmI6DYdqj/CXJRsZP3MN89dtp3njHG4pLeLOi4vp1q5ZvdSQ6o4ccS75+QyK2zflD1++MNnliMhxaIipjjXKzuK6AZ25bkBnFoS3M2HmGp6Zs5YJsyoZ3rsD48pKuLh7u4z+zP+sD7dSvX0f947slexSROQ0qUGcoYFdWvPLsYP4/qhzeGbOWv7w7jpeX/4uvTq24M6yYm4YWEiT3MzLHlIwn0jDp3MQdaRDyzy+dWUvZt43jF/c3J/sLOP+Fxdz0cPTePgv77N++75kl1hvduw9xF+XbuSGQQrmE2nIdARRx/IaZXNLaRduPq+IisptjJ+5hsfe+pDfvb2akX07Ma6smPO6tUnr4ac/BcF8uvZBpGFTg0gQM2NISVuGlLSlattenp69lmffW8erizfQr7AVd15czDUDCmick37vsCdWhOlT0JJzC1sluxQROQMaYqoHRW2acv+oc5jz/eH8+w3nsu9QLd+etJCyh2fw6Gsr2bxrf7JLrDNLqnewdP1OxXqLpAEdQdSjprk53H5hNz5/QVfeWbWF8TMr+dW0D/ifN1Zxbf/OjCsroV9Rw37XPSkUJjcni+sHdk52KSJyhtQgksDMuKRnPpf0zGfNlj08OauSSaEwL86vprRbG+4sK2Zk304NLiRw/6FaXl6wnqv6dqJ1UwXziTR0ahBJVtK+GQ9c15dvXXk2k0JVPDmrkrv/OJ+CVnn8w0XduO38rg0mBfVvHwfz6cppkXSgK6lTTO0RZ8b7mxk/aw0zV20lr1EWNw4q5M6LS+jVKbVDAo8G871971BlVIk0EEm7ktrMRgK/ArKBx9394WMefxQYGsw2BTq4e2szGwo8GrVqb2Csu7+cyHpTQXaWcUWfjlzRpyMrNu5iwqw1vDivmmffC3Nx93aMKythWO8OKRd+dzSY72vDeqo5iKSJhB1BmFk2sBIYAVQBFcBt7r7sOOvfAwxy9y8es7wtsAoocve9x3u9dDmCiGXbnoM8VxHmqdmVbNixn65tm3LHxcXcUlpEy7xGyS4PgEdfW8mvp3/A2/cOpahN02SXIyJxOtERRCLPgg4BVrn7anc/CDwHXH+C9W8Dno2x/GbgLydqDumuTbNcvnp5d96+dyi/+dxgOrRozL+9soyLfjqNByYvZc2WPUmt78gR5/m5VZR1b6/mIJJGEjnEVAiEo+argAtirWhm3YASYHqMh8cCjxxnu7uAuwC6du16JrU2CDnZWYzuX8Do/gUsrtrB+Jlr+MO7kZDAob3yGVdWwiU929f7VdozP9xC9fZ9fO/q3vX6uiKSWKnyOcqxwPPuXhu90MwKgH7A1Fgbuftj7l7q7qX5+fn1UGbq6FfUikfGDGTmfcP4+vCeLK7ewReeeI8Rj77FM3PWsvdg/d2jojxURasmjbiyT8d6e00RSbxENohqIPpy2qJgWSxjiT28dCvwkrvrhtDH0aFFHt8ccTYz7xvGI7cOoEmjbH7w8hIu/Ok0HpqynKptiR2Z2773IFOXbuSGgZ0VzPxZfLcAAAqnSURBVCeSZhI5xFQB9DSzEiKNYSzwuWNXMrPeQBtgdoznuA24P4E1po3GOdncNLiIGwcVMnftNsbPrOTxd9bwu7dXc2WfSEjgkJK2dT789KcF6yPBfIrWEEk7CWsQ7n7YzO4mMjyUDTzh7kvN7EEg5O6Tg1XHAs/5MR+nMrNiIkcgbyaqxnRkZpQWt6W0uC3rt+/jqdlrea5iHX9dupE+BS0ZV1bMtQPq7t3+xIowfTu3pG/nhh0RIiKfpgvlMsC+g7W8vKCa8TPXsHLTbto1y+XzF3Tl8xd2o2PLvNN+3iXVO7jmv97hwev78oWLiuuuYBGpN7rlaIZrkpvNbUO6Mvb8Lsz6cCvjZ67hv2as4n/e+JDR/QsYV1bCwC6tT/l5y48G8w0oTEDVIpJsahAZxMwo69Gesh7tqdyyh6dmr6U8FOZPC9YzqGtrxpWVcPW5nWgUR0jg/kO1vDy/mpF9O9GqaWpcrCcidUsNIkMVt2/GD6/tw7euPJvnQ2EmzKrka8/Op2PLxvzDhd24bUhX2jVvfNztpy7dyM79h3XXOJE0pnMQAkSuhn5j5WbGz6zk7Q+2kJuTxQ0DI/eoOKeg5afW//zjc6jcslfBfCINnM5ByEllZRnDendkWO+OfLBpF+NnVfLivCrKQ1VcUNKWcWUljOjTkewsI/zRXmau2so3rlAwn0g6U4OQT+nZsQU/vbEf917Vi4kVYZ6avZZ/emYuRW2acMdFxWzcuR8zuPk83fdBJJ2pQchxtW6ay1cu686XPlPCa8s2MX5mJT+ZshyAS3oqmE8k3alByEnlZGdxdb8Cru5XwJLqHbwwr4obB+mjrSLpTg1CTsm5ha04t1BXTYtkglRJcxURkRSjBiEiIjGpQYiISExqECIiEpMahIiIxKQGISIiMalBiIhITGoQIiISU9qkuZpZDbA2mG0PbEliOcmUyfsOmb3/mbzvkNn7fyb73s3d82M9kDYNIpqZhY4XX5vuMnnfIbP3P5P3HTJ7/xO17xpiEhGRmNQgREQkpnRtEI8lu4AkyuR9h8ze/0zed8js/U/IvqflOQgRETlz6XoEISIiZ0gNQkREYmrwDcLMnjCzzWa2JGpZWzN7zcw+CL63SWaNiWJmXcxshpktM7OlZvb1YHna77+Z5ZnZe2a2MNj3HwfLS8zsXTNbZWYTzSw32bUmipllm9l8M3slmM+kfa80s8VmtsDMQsGytP+5BzCz1mb2vJm9b2bLzeyiRO17g28QwARg5DHL7gOmuXtPYFown44OA9929z7AhcC/mFkfMmP/DwDD3H0AMBAYaWYXAj8DHnX3HsA24EtJrDHRvg4sj5rPpH0HGOruA6M+/58JP/cAvwL+6u69gQFEfgYSs+/u3uC/gGJgSdT8CqAgmC4AViS7xnr6d/gTMCLT9h9oCswDLiByNWlOsPwiYGqy60vQPhcFfwiGAa8Alin7HuxfJdD+mGVp/3MPtALWEHzAKNH7ng5HELF0dPcNwfRGoGMyi6kPZlYMDALeJUP2PxhiWQBsBl4DPgS2u/vhYJUqoDBZ9SXYL4F7gSPBfDsyZ98BHPibmc01s7uCZZnwc18C1ADjg+HFx82sGQna93RtEB/zSEtN68/ymllz4AXgG+6+M/qxdN5/d69194FE3k0PAXonuaR6YWbXAJvdfW6ya0miz7j7YOBqIkOrl0Y/mMY/9znAYOB/3X0QsIdjhpPqct/TtUFsMrMCgOD75iTXkzBm1ohIc/iDu78YLM6Y/Qdw9+3ADCLDKq3NLCd4qAioTlphiVMGXGdmlcBzRIaZfkVm7DsA7l4dfN8MvETkDUIm/NxXAVXu/m4w/zyRhpGQfU/XBjEZuCOYvoPI2HzaMTMDfg8sd/dHoh5K+/03s3wzax1MNyFy7mU5kUZxc7BaWu67u9/v7kXuXgyMBaa7++fJgH0HMLNmZtbi6DRwJbCEDPi5d/eNQNjMegWLhgPLSNC+N/grqc3sWeByInG3m4AfAS8D5UBXIhHgt7r7R8mqMVHM7DPA28Bi/j4W/X0i5yHSev/NrD/wJJBN5I1Oubs/aGZnEXlX3RaYD9zu7geSV2limdnlwHfc/ZpM2fdgP18KZnOAP7r7T8ysHWn+cw9gZgOBx4FcYDUwjuB3gDre9wbfIEREJDHSdYhJRETOkBqEiIjEpAYhIiIxqUGIiEhMahAiIhKTGoQ0KGZWGyR4LjSzeWZ28UnWb21m/xzH875hZhl5w/vjCRJT2ye7DkkeNQhpaPZ5JMFzAHA/8NBJ1m8NnLRBJEvUlc8iKUcNQhqylkRirTGz5mY2LTiqWGxm1wfrPAx0D446fhGs+71gnYVm9nDU890S3GNipZldEqybbWa/MLMKM1tkZl8JlheY2VvB8y45un604B34z4PXes/MegTLJ5jZ/5nZu8DPzWygmc0Jnv+lo1n+ZtbDzF6POlrqHiz/blQ9R++D0czMXg3WXWJmY4LlD1vkfiGLzOw/gmX5ZvZC8BwVZlYWLG9nZn+zyP01HieSECuZLNnxtfrS16l8AbXAAuB9YAdwXrA8B2gZTLcHVhH5A1fMJ6PgrwZmAU2D+bbB9zeA/wymRwGvB9N3AT8IphsDISKJmt8G/jVYng20iFFrZdQ6XwBeCaYnEInozg7mFwGXBdMPAr8Mpt8Fbgym84jEml9J5Ab1RuQN3ivApcBngd9FvXYrIgmvK/j7BbGtg+9/JBJ2B5Erb5cH078GfhhMjyYS+Nb+2P3SV+Z86fBWGpp9HklwxcwuAp4ys3OJ/MH8aZDqeYRI1HWsyOMrgPHuvhfAPxlHcDTscC6RxgKRP8j9zexoxlEroCdQATwRhCW+7O4LjlPvs1HfH41aPsnda82sFZE/3G8Gy58EJgVZQ4Xu/lJQ5/5gn68MapofrN88qOdt4D/N7GdEGtHbwfDVfuD3Frnr3CtR/wZ9IlFeALQMEoEvBW4KXu9VM9t2nH2SDKEGIQ2Wu88OTqLmE3nXn0/kiOJQkHSad4pPeTS3qJa//24YcI+7Tz125aAZjQYmmNkj7v5UrDKPM73nFGv7+GWBh9z9tzHqGUzk3+HfzWyaR7KphhAJdLsZuJtI8msWcOHRphO1/WmWJOlK5yCkwTKz3kSGd7YSeWe/OWgOQ4FuwWq7gBZRm70GjDOzpsFztD3Jy0wFvhocKWBmZwfj/d2ATe7+OyLBaYOPs/2YqO+zj33Q3XcA26LOYfwD8Ka77wKqzOyG4HUbBzVPBb4YvOPHzArNrIOZdQb2uvszwC+AwcE6rdx9CvBNIrenBPgbcM/RGoLwN4C3gM8Fy64G0vKezhI/HUFIQ9PEIneRg8i76TuCoZo/AH82s8VEzhO8D+DuW81sppktAf7i7t8N/iCGzOwgMIVIAu7xPE5kuGmeRd5i1wA3EEkQ/q6ZHQJ2EznHEEsbM1tE5OjktuOscwfwf0EDOJrOCZFm8VszexA4BNzi7n8zs3OA2cE7/t3A7UAP4BdmdiRY96tEGuOfzCwv+Lf6VvC8XwN+E9SVQ6Qx/BPwY+BZM1tK5DzNuhP8u0gGUJqrSIIEw1yl7r4l2bWInA4NMYmISEw6ghARkZh0BCEiIjGpQYiISExqECIiEpMahIiIxKQGISIiMf1/csHsz2t/FKcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}